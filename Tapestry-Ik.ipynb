{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4cf4d737-6771-4b57-bd17-8cd8273e611c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3db6e214-ce35-4257-b9f0-61e73bdfaa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"article_extraction.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c8a55e0a-cdcf-46df-a91c-2b8f9c29b48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsArticleExtractor:\n",
    "    def __init__(self, excel_file, output_dir=\"extracted_articles\"):\n",
    "        \"\"\"\n",
    "        Initialize the extractor with the Excel file path and output directory.\n",
    "        \n",
    "        Args:\n",
    "            excel_file (str): Path to the Excel file with article metadata\n",
    "            output_dir (str): Root directory to save extracted articles\n",
    "        \"\"\"\n",
    "        self.excel_file = excel_file\n",
    "        self.output_dir = output_dir\n",
    "        \n",
    "        # Extract source and topic from filename\n",
    "        filename = os.path.basename(excel_file)\n",
    "        parts = filename.replace('.xlsx', '').split('_')\n",
    "        self.source = parts[0]  # e.g., ST, BBC, CNN\n",
    "        \n",
    "        # Extract topic - handle different naming patterns\n",
    "        if 'Articles' in parts:\n",
    "            articles_index = parts.index('Articles')\n",
    "            self.topic = '_'.join(parts[1:articles_index])\n",
    "        elif 'Processed' in parts:\n",
    "            processed_index = parts.index('Processed')\n",
    "            self.topic = '_'.join(parts[1:processed_index])\n",
    "        else:\n",
    "            self.topic = '_'.join(parts[1:3])  # Fallback\n",
    "            \n",
    "        # Create output directories\n",
    "        self.create_directories()\n",
    "        \n",
    "        # Headers for requests to mimic browser\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "            'Cache-Control': 'max-age=0',\n",
    "        }\n",
    "    \n",
    "    def create_directories(self):\n",
    "        \"\"\"Create the directory structure for the extracted articles\"\"\"\n",
    "        # Main directories\n",
    "        Path(self.output_dir).mkdir(exist_ok=True)\n",
    "        \n",
    "        # By source, topic, and relevance\n",
    "        Path(f\"{self.output_dir}/by_source/{self.source}\").mkdir(parents=True, exist_ok=True)\n",
    "        Path(f\"{self.output_dir}/by_topic/{self.topic}\").mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Relevance directories\n",
    "        for r in range(1, 6):\n",
    "            Path(f\"{self.output_dir}/by_relevance/R{r}_{'Most_' if r == 5 else 'Highly_' if r == 4 else 'Somewhat_' if r == 2 else 'Least_' if r == 1 else ''}Relevant\").mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def clean_and_format_content(self, content_list):\n",
    "        \"\"\"\n",
    "        Clean and format the extracted content, handling sub-headers and paragraphs\n",
    "        Args:\n",
    "            content_list (list): List of extracted content elements\n",
    "        Returns:\n",
    "            str: Cleaned and formatted content\n",
    "        Note to Claude: DO NOT AMEND THIS DEF / FUNCTION\n",
    "        \"\"\"\n",
    "        # Cleaned and formatted content elements\n",
    "        formatted_content = []\n",
    "        \n",
    "        # Previous element tracking to manage formatting\n",
    "        last_type = None\n",
    "        \n",
    "        for item in content_list:\n",
    "            # Clean the item (remove extra whitespace)\n",
    "            cleaned_item = re.sub(r'\\s+', ' ', item.strip())\n",
    "            \n",
    "            # Skip completely empty items\n",
    "            if not cleaned_item:\n",
    "                continue\n",
    "            \n",
    "            # Identify item type\n",
    "            if cleaned_item.startswith('##'):\n",
    "                # Sub-header handling\n",
    "                sub_header = cleaned_item.replace('##', '').strip()\n",
    "                \n",
    "                # Add extra newlines around sub-headers for clear separation\n",
    "                formatted_content.append(f\"\\n\\n## {sub_header}\\n\\n\")\n",
    "                last_type = 'subheader'\n",
    "            else:\n",
    "                # Regular paragraph handling\n",
    "                if last_type == 'paragraph':\n",
    "                    # Add newline between consecutive paragraphs\n",
    "                    formatted_content.append(f\"\\n{cleaned_item}\")\n",
    "                else:\n",
    "                    formatted_content.append(cleaned_item)\n",
    "                last_type = 'paragraph'\n",
    "        \n",
    "        # Join content with careful formatting\n",
    "        return \"\\n\".join(formatted_content).strip()\n",
    "                          \n",
    "    def extract_article_text(self, soup, url):\n",
    "        \"\"\"\n",
    "        Extract the article text from the soup object with hierarchical preservation and list handling\n",
    "        \n",
    "        Args:\n",
    "            soup (BeautifulSoup): Parsed HTML content\n",
    "            url (str): URL of the article\n",
    "        \n",
    "        Returns:\n",
    "            str: Formatted article content\n",
    "        \"\"\"\n",
    "        # List to collect structured content\n",
    "        structured_content = []\n",
    "        site = urlparse(url).netloc\n",
    "        \n",
    "        # Check if it's an archive buttons URL\n",
    "        if 'archivebuttons.com' in site:\n",
    "            # Detailed logging for debugging\n",
    "            logger.debug(f\"Extracting content from Archive Buttons URL: {url}\")\n",
    "            \n",
    "            # Try multiple methods to find article content\n",
    "            content_areas = [\n",
    "                soup.select_one('div.text'),  # Specific class I noticed in the screenshot\n",
    "                soup.select_one('div[style*=\"color:rgb(0, 0, 0)\"]'),  # Selector based on text color\n",
    "                soup.find('div', class_=lambda x: x and 'content' in x.lower()),\n",
    "                soup.select_one('body'),  # Fallback to entire body\n",
    "            ]\n",
    "        \n",
    "            # Process the first non-None content area\n",
    "            for area in content_areas:\n",
    "                if area:\n",
    "                    # Track current header hierarchy\n",
    "                    current_headers = {\n",
    "                        'h1': None,\n",
    "                        'h2': None,\n",
    "                        'h3': None,\n",
    "                        'h4': None,\n",
    "                        'h5': None,\n",
    "                        'h6': None\n",
    "                    }\n",
    "                    \n",
    "                    # Track list nesting\n",
    "                    current_list_level = 0\n",
    "                    \n",
    "                    # Find all elements maintaining order\n",
    "                    elements = area.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'ul', 'ol'])\n",
    "                    \n",
    "                    for element in elements:\n",
    "                        # Process headers\n",
    "                        if element.name.startswith('h'):\n",
    "                            header_text = element.get_text().strip()\n",
    "                            header_level = int(element.name[1])\n",
    "                            \n",
    "                            # Reset lower-level headers\n",
    "                            for level in current_headers:\n",
    "                                if int(level[1]) > header_level:\n",
    "                                    current_headers[level] = None\n",
    "                            \n",
    "                            # Store current header\n",
    "                            current_headers[element.name] = header_text\n",
    "                            \n",
    "                            # Construct header hierarchy\n",
    "                            header_levels = [current_headers[f'h{i}'] for i in range(1, header_level + 1) if current_headers[f'h{i}']]\n",
    "                            header_string = \" > \".join(filter(None, header_levels))\n",
    "                            \n",
    "                            # Add header with full context\n",
    "                            structured_content.append(f\"## {header_string}\")\n",
    "                            \n",
    "                            # Reset list level when a new header is encountered\n",
    "                            current_list_level = 0\n",
    "                        \n",
    "                        # Process lists\n",
    "                        elif element.name in ['ul', 'ol']:\n",
    "                            current_list_level += 1\n",
    "                            list_items = element.find_all('li', recursive=False)\n",
    "                            \n",
    "                            for item in list_items:\n",
    "                                # Indent based on list nesting level\n",
    "                                indent = '  ' * (current_list_level - 1)\n",
    "                                list_item_text = item.get_text().strip()\n",
    "                                \n",
    "                                # Check if the list item contains nested lists\n",
    "                                nested_lists = item.find_all(['ul', 'ol'], recursive=False)\n",
    "                                \n",
    "                                # Add the list item\n",
    "                                if list_item_text:\n",
    "                                    structured_content.append(f\"{indent}• {list_item_text}\")\n",
    "                            \n",
    "                                # Process nested lists recursively\n",
    "                                if nested_lists:\n",
    "                                    for nested_list in nested_lists:\n",
    "                                        nested_items = nested_list.find_all('li', recursive=False)\n",
    "                                        for nested_item in nested_items:\n",
    "                                            nested_item_text = nested_item.get_text().strip()\n",
    "                                            if nested_item_text:\n",
    "                                                structured_content.append(f\"{indent}  • {nested_item_text}\")\n",
    "                        \n",
    "                        # Process paragraphs\n",
    "                        elif element.name == 'p':\n",
    "                            para_text = element.get_text().strip()\n",
    "                            if para_text:\n",
    "                                structured_content.append(para_text)\n",
    "                    \n",
    "                    # If content was found, break the loop\n",
    "                    if structured_content:\n",
    "                        break\n",
    "            \n",
    "            logger.debug(f\"Extracted content from Archive Buttons: {structured_content}\")\n",
    "        \n",
    "        # Original Straits Times website handling\n",
    "        elif 'straitstimes.com' in site:\n",
    "            # Headline and Subheadline extraction\n",
    "            headlines = []\n",
    "            h1_elements = soup.select('h1.headline, h1.font-secondary-header')\n",
    "            if h1_elements:\n",
    "                headlines.append(h1_elements[0].get_text().strip())\n",
    "            \n",
    "            h2_elements = soup.select('h2.font-secondary-header')\n",
    "            if h2_elements:\n",
    "                headlines.append(h2_elements[0].get_text().strip())\n",
    "            \n",
    "            # Add headlines/subheadlines to content\n",
    "            structured_content.extend(headlines)\n",
    "            \n",
    "            # Article content extraction\n",
    "            article_content = soup.select('section.article-content')\n",
    "            if article_content:\n",
    "                for section in article_content:\n",
    "                    # Track current header level\n",
    "                    current_headers = {\n",
    "                        'h1': None,\n",
    "                        'h2': None,\n",
    "                        'h3': None,\n",
    "                        'h4': None,\n",
    "                        'h5': None,\n",
    "                        'h6': None\n",
    "                    }\n",
    "                \n",
    "                    # Track list nesting\n",
    "                    current_list_level = 0\n",
    "                    \n",
    "                    # Collect all elements in order\n",
    "                    elements = section.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'ul', 'ol'])\n",
    "                    \n",
    "                    for element in elements:\n",
    "                        # Process headers\n",
    "                        if element.name.startswith('h'):\n",
    "                            header_text = element.get_text().strip()\n",
    "                            header_level = int(element.name[1])\n",
    "                            \n",
    "                            # Reset lower-level headers\n",
    "                            for level in current_headers:\n",
    "                                if int(level[1]) > header_level:\n",
    "                                    current_headers[level] = None\n",
    "                            \n",
    "                            # Store current header\n",
    "                            current_headers[element.name] = header_text\n",
    "                            \n",
    "                            # Construct header hierarchy\n",
    "                            header_levels = [current_headers[f'h{i}'] for i in range(1, header_level + 1) if current_headers[f'h{i}']]\n",
    "                            header_string = \" > \".join(filter(None, header_levels))\n",
    "                            \n",
    "                            # Add header with full context\n",
    "                            structured_content.append(f\"## {header_string}\")\n",
    "                            \n",
    "                            # Reset list level when a new header is encountered\n",
    "                            current_list_level = 0\n",
    "                        \n",
    "                        # Process lists\n",
    "                        elif element.name in ['ul', 'ol']:\n",
    "                            current_list_level += 1\n",
    "                            list_items = element.find_all('li', recursive=False)\n",
    "                        \n",
    "                            for item in list_items:\n",
    "                                # Indent based on list nesting level\n",
    "                                indent = '  ' * (current_list_level - 1)\n",
    "                                list_item_text = item.get_text().strip()\n",
    "                                \n",
    "                                # Check if the list item contains nested lists\n",
    "                                nested_lists = item.find_all(['ul', 'ol'], recursive=False)\n",
    "                                \n",
    "                                # Add the list item\n",
    "                                if list_item_text:\n",
    "                                    structured_content.append(f\"{indent}• {list_item_text}\")\n",
    "                                \n",
    "                                # Process nested lists recursively\n",
    "                                if nested_lists:\n",
    "                                    for nested_list in nested_lists:\n",
    "                                        nested_items = nested_list.find_all('li', recursive=False)\n",
    "                                        for nested_item in nested_items:\n",
    "                                            nested_item_text = nested_item.get_text().strip()\n",
    "                                            if nested_item_text:\n",
    "                                                structured_content.append(f\"{indent}  • {nested_item_text}\")\n",
    "                    \n",
    "                        # Process paragraphs\n",
    "                        elif element.name == 'p':\n",
    "                            # Skip empty or ad paragraphs\n",
    "                            parent_id = element.parent.get('id', '')\n",
    "                            p_text = element.get_text().strip()\n",
    "                            if p_text and not (parent_id.startswith('dfp-ad') or parent_id.startswith('sph_')):\n",
    "                                # Check for paragraph-specific selectors used in Straits Times\n",
    "                                if element.get('class') and 'paragraph-base' in element.get('class'):\n",
    "                                    structured_content.append(p_text)\n",
    "                                elif not element.get('class'):\n",
    "                                    structured_content.append(p_text)\n",
    "\n",
    "########$###################################################################        \n",
    "        # BBC website handling\n",
    "        elif 'bbc.com' in site or 'bbc.co.uk' in site:\n",
    "            # Headline extraction - only extract once\n",
    "            headlines = []\n",
    "            h1_elements = soup.select('h1.sc-18f6de06-0, h1.ssrcss-1q0x1qg-Paragraph')\n",
    "            if h1_elements:\n",
    "                headlines.append(h1_elements[0].get_text().strip())\n",
    "            \n",
    "            # Add headlines to content\n",
    "            structured_content.extend(headlines)\n",
    "            \n",
    "            # Track current header hierarchy\n",
    "            current_headers = {\n",
    "                'h1': headlines[0] if headlines else None,\n",
    "                'h2': None,\n",
    "                'h3': None,\n",
    "                'h4': None,\n",
    "                'h5': None,\n",
    "                'h6': None\n",
    "            }\n",
    "\n",
    "            # Track list nesting\n",
    "            current_list_level = 0\n",
    "            \n",
    "            # Important fix: Choose only ONE content area to extract from\n",
    "            # Previously multiple areas could contain duplicated content\n",
    "            content_area = None\n",
    "            \n",
    "            # Try to find the main content area in order of preference\n",
    "            potential_areas = [\n",
    "                soup.select_one('main#main-content'),\n",
    "                soup.select_one('div.article-body'),\n",
    "                soup.select_one('div[data-component=\"text-block\"]').parent if soup.select_one('div[data-component=\"text-block\"]') else None,\n",
    "                soup.select_one('div[data-component=\"rich-text-block\"]').parent if soup.select_one('div[data-component=\"rich-text-block\"]') else None\n",
    "            ]\n",
    "            \n",
    "            # Use the first valid content area\n",
    "            for area in potential_areas:\n",
    "                if area:\n",
    "                    content_area = area\n",
    "                    break\n",
    "            \n",
    "            if content_area:\n",
    "                # Collect all elements maintaining order\n",
    "                elements = content_area.find_all(['h2', 'h3', 'h4', 'h5', 'h6', 'p', 'ul', 'ol', 'b'])\n",
    "        \n",
    "                for element in elements:\n",
    "                    # Process headers\n",
    "                    if element.name.startswith('h') and element.name != 'h1':\n",
    "                        header_text = element.get_text().strip()\n",
    "                        header_level = int(element.name[1])\n",
    "                        \n",
    "                        # Reset lower-level headers\n",
    "                        for level in current_headers:\n",
    "                            if int(level[1]) > header_level:\n",
    "                                current_headers[level] = None\n",
    "                        \n",
    "                        # Store current header\n",
    "                        current_headers[element.name] = header_text\n",
    "                        \n",
    "                        # Construct header hierarchy\n",
    "                        header_levels = [current_headers[f'h{i}'] for i in range(1, header_level + 1) if current_headers[f'h{i}']]\n",
    "                        header_string = \" > \".join(filter(None, header_levels))\n",
    "                        \n",
    "                        # Add header with full context\n",
    "                        structured_content.append(f\"## {header_string}\")\n",
    "                        \n",
    "                        # Reset list level when a new header is encountered\n",
    "                        current_list_level = 0\n",
    "            \n",
    "                    # Process paragraphs - handle multiple BBC formats\n",
    "                    elif element.name == 'p':\n",
    "                        # Get class attribute as a string or empty list if no class\n",
    "                        classes = element.get('class') or []\n",
    "                        class_str = ' '.join(classes) if classes else ''\n",
    "                        \n",
    "                        # Check for any of the known BBC paragraph patterns:\n",
    "                        # 1. Classes that start with 'sc-' (older pattern)\n",
    "                        # 2. Classes that contain '-Paragraph' (newer pattern)\n",
    "                        # 3. Classes that start with 'ssrcss-' (newer pattern)\n",
    "                        # 4. No class at all (fallback)\n",
    "                        has_sc_class = any(isinstance(cls, str) and cls.startswith('sc-') for cls in classes)\n",
    "                        has_paragraph_class = 'Paragraph' in class_str\n",
    "                        has_ssrcss_class = any(isinstance(cls, str) and cls.startswith('ssrcss-') for cls in classes)\n",
    "                        \n",
    "                        if has_sc_class or has_paragraph_class or has_ssrcss_class or not classes:\n",
    "                            p_text = element.get_text().strip()\n",
    "                            if p_text:\n",
    "                                structured_content.append(p_text)\n",
    "            \n",
    "                    # Process bold text (sometimes contains important content)\n",
    "                    elif element.name == 'b':\n",
    "                        b_text = element.get_text().strip()\n",
    "                        if b_text and len(b_text) > 20:  # Only include substantial bold text\n",
    "                            structured_content.append(b_text)\n",
    "                    \n",
    "                    # Process lists\n",
    "                    elif element.name in ['ul', 'ol']:\n",
    "                        current_list_level += 1\n",
    "                        list_items = element.find_all('li', recursive=False)\n",
    "                        \n",
    "                        for item in list_items:\n",
    "                            # Indent based on list nesting level\n",
    "                            indent = '  ' * (current_list_level - 1)\n",
    "                            list_item_text = item.get_text().strip()\n",
    "                            \n",
    "                            # Check if the list item contains nested lists\n",
    "                            nested_lists = item.find_all(['ul', 'ol'], recursive=False)\n",
    "                            \n",
    "                            # Add the list item\n",
    "                            if list_item_text:\n",
    "                                structured_content.append(f\"{indent}• {list_item_text}\")\n",
    "                    \n",
    "                            # Process nested lists recursively\n",
    "                            if nested_lists:\n",
    "                                for nested_list in nested_lists:\n",
    "                                    nested_items = nested_list.find_all('li', recursive=False)\n",
    "                                    for nested_item in nested_items:\n",
    "                                        nested_item_text = nested_item.get_text().strip()\n",
    "                                        if nested_item_text:\n",
    "                                            structured_content.append(f\"{indent}  • {nested_item_text}\")\n",
    "            \n",
    "            # Additional fix: Deduplicate content\n",
    "            # Sometimes the same content might still be extracted twice from different elements\n",
    "            deduplicated_content = []\n",
    "            seen_content = set()\n",
    "            \n",
    "            for item in structured_content:\n",
    "                # Skip this item if it's identical to the previous one\n",
    "                if item in seen_content:\n",
    "                    continue\n",
    "                    \n",
    "                deduplicated_content.append(item)\n",
    "                seen_content.add(item)\n",
    "            \n",
    "            # Replace the original list with deduplicated content\n",
    "            structured_content = deduplicated_content\n",
    "            \n",
    "            # Logging\n",
    "            logger.debug(f\"BBC Article Extraction Debug:\")\n",
    "            logger.debug(f\"Total content items after deduplication: {len(structured_content)}\")\n",
    "            \n",
    "########$###################################################################\n",
    "        # Optimized Channel News Asia (CNA) website handling with excluded sections\n",
    "        elif 'channelnewsasia.com' in site or 'cnalifestyle.channelnewsasia.com' in site:\n",
    "            # Headline extraction\n",
    "            headlines = []\n",
    "            h1_elements = soup.select('h1.layout__title, h1.h1--page-title, div.article-header h1')\n",
    "            if h1_elements:\n",
    "                headlines.append(h1_elements[0].get_text().strip())\n",
    "            \n",
    "            # Add headlines to content\n",
    "            structured_content.extend(headlines)\n",
    "            \n",
    "            # Define termination markers and AI disclaimers once\n",
    "            termination_markers = [\n",
    "                \"Subscribe to our Chief Editor's Week in Review\",\n",
    "                \"Get our pick of top stories and thought-provoking articles in your inbox\"\n",
    "            ]\n",
    "            ai_disclaimers = [\"This audio is generated by an AI tool\"]\n",
    "            \n",
    "            # Define sections to exclude\n",
    "            excluded_sections = [\n",
    "                'div.referenced-card',    # \"Related:\" section with links to other articles\n",
    "                '.elementor-grid-item',   # Other potential \"related content\" containers\n",
    "                '.media-object',          # Media object containers that often contain related articles\n",
    "                '.teaser',                # Article teasers\n",
    "                'div.recommended-articles', # Recommended articles section\n",
    "                '.read-more',             # \"Read more\" sections\n",
    "                '.article-tags',          # Article tags\n",
    "                '.article-social-share'   # Social share buttons\n",
    "            ]\n",
    "    \n",
    "            # Check if an element is inside an excluded section\n",
    "            def is_in_excluded_section(element):\n",
    "                # Check if the element has any of the excluded classes\n",
    "                for cls in excluded_sections:\n",
    "                    # Strip the CSS selector prefix if present\n",
    "                    class_name = cls.replace('.', '')\n",
    "                    if class_name in (element.get('class') or []):\n",
    "                        return True\n",
    "                        \n",
    "                # Check if any parent is in an excluded section\n",
    "                parent = element.parent\n",
    "                while parent and parent.name != 'body':\n",
    "                    if parent.get('class'):\n",
    "                        for cls in excluded_sections:\n",
    "                            class_name = cls.replace('.', '')\n",
    "                            if class_name in parent.get('class'):\n",
    "                                return True\n",
    "                    parent = parent.parent\n",
    "                    \n",
    "                return False\n",
    "            \n",
    "            # Function to check if text should be skipped or signals termination\n",
    "            def should_skip_or_terminate(text):\n",
    "                if any(marker in text for marker in termination_markers):\n",
    "                    logger.info(\"Found newsletter subscription marker - terminating extraction\")\n",
    "                    return \"terminate\"\n",
    "                elif any(disclaimer in text for disclaimer in ai_disclaimers):\n",
    "                    return \"skip\"\n",
    "                return False\n",
    "    \n",
    "            # Find all potential content containers in one go\n",
    "            content_containers = soup.select(\n",
    "                'section[data-title=\"Content\"], '\n",
    "                'div.layout_region, '\n",
    "                'div.article-body, '\n",
    "                'div.article-content, '\n",
    "                'div.content-wrapper, '\n",
    "                'div.text-long, '\n",
    "                'div.text'\n",
    "            )\n",
    "            \n",
    "            # Extract text content from all containers\n",
    "            all_content = []\n",
    "            terminate_extraction = False\n",
    "            \n",
    "            for container in content_containers:\n",
    "                if terminate_extraction:\n",
    "                    break\n",
    "                    \n",
    "                # Process all text elements (paragraphs and spans)\n",
    "                text_elements = container.select('p, span')\n",
    "                \n",
    "                for element in text_elements:\n",
    "                    # Skip elements that are likely not actual content\n",
    "                    if element.parent.name == 'p' and element.name == 'span':\n",
    "                        # Skip spans inside paragraphs as we'll get their text from the paragraph\n",
    "                        continue\n",
    "                        \n",
    "                    # Skip elements in excluded sections\n",
    "                    if is_in_excluded_section(element):\n",
    "                        continue\n",
    "                \n",
    "                    text = element.get_text().strip()\n",
    "                    \n",
    "                    # Skip empty elements\n",
    "                    if not text or len(text) <= 1:\n",
    "                        continue\n",
    "                        \n",
    "                    # Check if we should skip or terminate\n",
    "                    check_result = should_skip_or_terminate(text)\n",
    "                    if check_result == \"terminate\":\n",
    "                        terminate_extraction = True\n",
    "                        break\n",
    "                    elif check_result == \"skip\":\n",
    "                        continue\n",
    "                        \n",
    "                    # Add content if it's not a duplicate of the last item\n",
    "                    if not all_content or text != all_content[-1]:\n",
    "                        all_content.append(text)\n",
    "                \n",
    "                # Process headers in the container but exclude those in excluded sections\n",
    "                if not terminate_extraction:\n",
    "                    headers = container.select('h2, h3, h4, h5, h6')\n",
    "                    for header in headers:\n",
    "                        if is_in_excluded_section(header):\n",
    "                            continue\n",
    "                    \n",
    "                        header_text = header.get_text().strip()\n",
    "                        if header_text:\n",
    "                            structured_content.append(f\"## {header_text}\")\n",
    "                    \n",
    "                    # Process lists in the container but exclude those in excluded sections\n",
    "                    lists = container.select('ul, ol')\n",
    "                    for list_elem in lists:\n",
    "                        if is_in_excluded_section(list_elem):\n",
    "                            continue\n",
    "                            \n",
    "                        list_items = list_elem.select('li')\n",
    "                        for item in list_items:\n",
    "                            item_text = item.get_text().strip()\n",
    "                            if item_text:\n",
    "                                structured_content.append(f\"• {item_text}\")\n",
    "            \n",
    "            # Add the collected content to structured_content\n",
    "            structured_content.extend(all_content)\n",
    "            \n",
    "            # Deduplicate content more efficiently\n",
    "            deduplicated_content = []\n",
    "            seen_content = set()\n",
    "            \n",
    "            for item in structured_content:\n",
    "                # Normalize whitespace for comparison\n",
    "                item_normalized = re.sub(r'\\s+', ' ', item).strip()\n",
    "                \n",
    "                # Skip if we've seen this exact content\n",
    "                if item_normalized in seen_content:\n",
    "                    continue\n",
    "            \n",
    "                # Skip if this content is part of another item we've already included\n",
    "                if any(item_normalized in seen and len(seen) > len(item_normalized) * 1.2 for seen in seen_content):\n",
    "                    continue\n",
    "                \n",
    "                deduplicated_content.append(item)\n",
    "                seen_content.add(item_normalized)\n",
    "            \n",
    "            # Replace the original list with deduplicated content\n",
    "            structured_content = deduplicated_content\n",
    "            \n",
    "            # Log extraction results\n",
    "            logger.debug(f\"CNA Article Extraction: Found {len(structured_content)} content items\")      \n",
    "\n",
    "########$###################################################################\n",
    "        # CNN website handling\n",
    "        elif 'cnn.com' in site or 'edition.cnn.com' in site:\n",
    "            # Headline extraction\n",
    "            headlines = []\n",
    "            h1_selectors = [\n",
    "                'h1.headline__text',\n",
    "                'h1.inline-placeholder',\n",
    "                'h1[data-editable=\"headlineText\"]',\n",
    "                'h1.headline'\n",
    "            ]\n",
    "            \n",
    "            for selector in h1_selectors:\n",
    "                h1_elements = soup.select(selector)\n",
    "                if h1_elements:\n",
    "                    headlines.append(h1_elements[0].get_text().strip())\n",
    "                    break\n",
    "            \n",
    "            # Add headlines to content\n",
    "            structured_content = headlines.copy() if headlines else []\n",
    "    \n",
    "            # Define sections to exclude\n",
    "            excluded_sections = [\n",
    "                # \"Up Next\" sections\n",
    "                'div[data-title=\"Up next\"]',\n",
    "                'section.layout_end',\n",
    "                'div.container_list-headlines-with-read-times',\n",
    "                'div[data-collapsed-text=\"Up next\"]',\n",
    "                # \"Most Read\" sections\n",
    "                'div[data-title=\"Most read\"]',\n",
    "                'div.container_list-headlines-ranked',\n",
    "                'div[data-collapsed-text=\"Most read\"]',\n",
    "                # Other sections to exclude\n",
    "                'div.ad-slot-dynamic',\n",
    "                'div.ad-feedback',\n",
    "                'div.zoneAds',\n",
    "                'div.related-content',\n",
    "                'div.related-articles',\n",
    "                '.newsletter-container'\n",
    "            ]\n",
    "    \n",
    "            # Function to check if element should be excluded\n",
    "            def is_excluded_element(element):\n",
    "                # Check element itself\n",
    "                if element.name == 'div' and element.get('data-title') in [\"Up next\", \"Most read\"]:\n",
    "                    return True\n",
    "                    \n",
    "                # Check for collapsed text markers\n",
    "                if element.get('data-collapsed-text') in [\"Up next\", \"Most read\"]:\n",
    "                    return True\n",
    "                    \n",
    "                # Check if the element is within an excluded section\n",
    "                parent = element.parent\n",
    "                while parent:\n",
    "                    # Check if parent has data-title attribute that matches excluded sections\n",
    "                    if parent.get('data-title') in [\"Up next\", \"Most read\"]:\n",
    "                        return True\n",
    "                        \n",
    "                    # Check if parent has data-collapsed-text attribute that matches excluded sections\n",
    "                    if parent.get('data-collapsed-text') in [\"Up next\", \"Most read\"]:\n",
    "                        return True\n",
    "                        \n",
    "                    # Check if parent has any of the excluded classes\n",
    "                    if parent.get('class'):\n",
    "                        parent_class = ' '.join(parent.get('class'))\n",
    "                        if any(x in parent_class for x in ['container_list-headlines-with-read-times', \n",
    "                                                          'container_list-headlines-ranked',\n",
    "                                                          'ad-slot-dynamic']):\n",
    "                            return True\n",
    "            \n",
    "                    parent = parent.parent\n",
    "                \n",
    "                return False\n",
    "            \n",
    "            # Find main content container\n",
    "            main_content = None\n",
    "            for selector in [\n",
    "                'section[data-tabcontent=\"content\"]',\n",
    "                'main.article__main',\n",
    "                'div.article__content',\n",
    "                'div.article_content',\n",
    "                'div.l-container',\n",
    "                'div.body-text'\n",
    "            ]:\n",
    "                container = soup.select_one(selector)\n",
    "                if container:\n",
    "                    main_content = container\n",
    "                    break\n",
    "            \n",
    "            # Process elements in the main content\n",
    "            if main_content:\n",
    "                # Get all text elements in the right order\n",
    "                article_elements = main_content.find_all(['h2', 'h3', 'h4', 'p', 'ul', 'ol'])\n",
    "                \n",
    "                for element in article_elements:\n",
    "                    # Skip elements in excluded sections\n",
    "                    if is_excluded_element(element):\n",
    "                        continue\n",
    "            \n",
    "                    # Process headers\n",
    "                    if element.name.startswith('h'):\n",
    "                        header_text = element.get_text().strip()\n",
    "                        if header_text and len(header_text) > 1:\n",
    "                            structured_content.append(f\"## {header_text}\")\n",
    "                    \n",
    "                    # Process paragraphs\n",
    "                    elif element.name == 'p':\n",
    "                        p_text = element.get_text().strip()\n",
    "                        \n",
    "                        # Skip empty paragraphs\n",
    "                        if not p_text or len(p_text) <= 1:\n",
    "                            continue\n",
    "                            \n",
    "                        # Skip paragraphs with \"This is a developing story\" text\n",
    "                        if \"This is a developing story\" in p_text:\n",
    "                            continue\n",
    "                            \n",
    "                        structured_content.append(p_text)\n",
    "                    \n",
    "                    # Process lists\n",
    "                    elif element.name in ['ul', 'ol']:\n",
    "                        list_items = element.find_all('li')\n",
    "                        for item in list_items:\n",
    "                            item_text = item.get_text().strip()\n",
    "                            if item_text:\n",
    "                                structured_content.append(f\"• {item_text}\")\n",
    "    \n",
    "            else:\n",
    "                # Fallback to simple paragraph extraction if main container not found\n",
    "                paragraphs = soup.find_all('p', class_=lambda x: x and ('paragraph' in x or 'article' in x))\n",
    "                \n",
    "                for p in paragraphs:\n",
    "                    # Skip elements in excluded sections\n",
    "                    if is_excluded_element(p):\n",
    "                        continue\n",
    "                        \n",
    "                    p_text = p.get_text().strip()\n",
    "                    if p_text and len(p_text) > 1:\n",
    "                        structured_content.append(p_text)\n",
    "            \n",
    "            # Deduplicate content\n",
    "            deduplicated_content = []\n",
    "            seen_content = set()\n",
    "            \n",
    "            for item in structured_content:\n",
    "                # Normalize whitespace for comparison\n",
    "                item_normalized = re.sub(r'\\s+', ' ', item).strip()\n",
    "                \n",
    "                # Skip if we've seen this exact content\n",
    "                if item_normalized in seen_content:\n",
    "                    continue\n",
    "                    \n",
    "                deduplicated_content.append(item)\n",
    "                seen_content.add(item_normalized)\n",
    "            \n",
    "            # Replace the original list with deduplicated content\n",
    "            structured_content = deduplicated_content\n",
    "            \n",
    "            # Log extraction results\n",
    "            logger.debug(f\"CNN Article Extraction: Found {len(structured_content)} content items\")\n",
    "        \n",
    "########$###################################################################\n",
    "\n",
    "        # Fallback if no content found\n",
    "        if not structured_content:\n",
    "            logger.warning(f\"No content extracted from URL: {url}\")\n",
    "            paragraphs = soup.find_all('p')\n",
    "            structured_content = [p.get_text().strip() for p in paragraphs if p.get_text().strip()]\n",
    "        \n",
    "        # Clean and format the extracted content\n",
    "        formatted_content = self.clean_and_format_content(structured_content)\n",
    "        \n",
    "        return formatted_content\n",
    "\n",
    "    # Enhance the journalist extraction function to better capture BBC journalist names\n",
    "    # This should be added to the extract_journalist method in the NewsArticleExtractor class\n",
    "    \n",
    "    def extract_journalist(self, soup, url):\n",
    "        \"\"\"\n",
    "        Extract the journalist name from the article\n",
    "        Args:\n",
    "            soup (BeautifulSoup): Parsed HTML content\n",
    "            url (str): URL of the article\n",
    "        Returns:\n",
    "            str or None: Extracted journalist name\n",
    "        \"\"\"\n",
    "        site = urlparse(url).netloc\n",
    "    \n",
    "        if 'bbc.com' in site or 'bbc.co.uk' in site:\n",
    "            # Method 1: Look for byline block section with journalist names\n",
    "            byline_block = soup.select_one('div[data-component=\"byline-block\"]')\n",
    "            if byline_block:\n",
    "                # Find all journalist name spans within the byline block\n",
    "                # These often have classes like \"sc-XXXXXX-7 kItaYD\" or similar naming patterns\n",
    "                journalist_spans = byline_block.select('span[class*=\"kItaYD\"], span[class*=\"byline-name\"]')\n",
    "                \n",
    "                if journalist_spans:\n",
    "                    # Collect all journalist names\n",
    "                    journalists = [span.get_text().strip() for span in journalist_spans if span.get_text().strip()]\n",
    "                    if journalists:\n",
    "                        # Join multiple journalists with commas\n",
    "                        return \", \".join(journalists)\n",
    "        \n",
    "            # Method 2: Look for specific byline containers with contributor details\n",
    "            byline_containers = soup.select('div[data-testid=\"byline-new\"], div[data-testid=\"byline-new-contributors\"]')\n",
    "            if byline_containers:\n",
    "                journalists = []\n",
    "                for container in byline_containers:\n",
    "                    # Look for name spans within the container\n",
    "                    # BBC often uses class names with patterns like \"sc-XXXXXX-7\" for journalist names\n",
    "                    name_spans = container.select('span[class*=\"-7\"]')\n",
    "                    for span in name_spans:\n",
    "                        name = span.get_text().strip()\n",
    "                        if name and len(name) > 2:  # Basic validation to ensure it looks like a name\n",
    "                            journalists.append(name)\n",
    "                \n",
    "                if journalists:\n",
    "                    return \", \".join(journalists)\n",
    "                \n",
    "            # Method 3: Look for byline sections with specific class patterns\n",
    "            byline_sections = soup.select('.byline, .byline-name, .author-name, div[class*=\"byline\"]')\n",
    "            for section in byline_sections:\n",
    "                text = section.get_text().strip()\n",
    "                if text and \"By \" in text:\n",
    "                    return text.replace(\"By \", \"\").strip()\n",
    "                elif text and len(text) > 2:\n",
    "                    return text\n",
    "\n",
    "\n",
    "        elif 'straitstimes.com' in site:\n",
    "            # Original Straits Times logic\n",
    "            byline_name_links = soup.select('a.text-blue-hyperlink.byline-name')\n",
    "            if byline_name_links:\n",
    "                for link in byline_name_links:\n",
    "                    journalist_name = link.get_text().strip()\n",
    "                    if journalist_name:\n",
    "                        return journalist_name\n",
    "        \n",
    "            # Look for photo captions with news agency or journalist name\n",
    "            photo_captions = soup.select('.photo-caption, figcaption')\n",
    "            for caption in photo_captions:\n",
    "                text = caption.get_text().strip()\n",
    "                \n",
    "                # Improved regex to match full names\n",
    "                name_matches = re.findall(r'\\b([A-Z][a-z]+\\s+[A-Z][a-z]+)\\b(?=\\s*/)', text)\n",
    "                if name_matches:\n",
    "                    return name_matches[0]\n",
    "                \n",
    "                # Check for news agencies\n",
    "                news_agencies = ['REUTERS', 'AP', 'AFP', 'ASSOCIATED PRESS']\n",
    "                for agency in news_agencies:\n",
    "                    if agency in text.upper():\n",
    "                        return agency\n",
    "    \n",
    "        # Final paragraph check for news agencies\n",
    "        paragraphs = soup.select('p.paragraph-base, p')\n",
    "        if paragraphs:\n",
    "            last_paragraph = paragraphs[-1].get_text().strip()\n",
    "            news_agencies = ['REUTERS', 'AP', 'AFP', 'ASSOCIATED PRESS', 'BBC News']\n",
    "            for agency in news_agencies:\n",
    "                if agency in last_paragraph:\n",
    "                    return agency\n",
    "\n",
    "########$#######################################################################################\n",
    "        # Optimized Channel News Asia (CNA) extraction with proper heading placement\n",
    "        elif 'channelnewsasia.com' in site or 'cnalifestyle.channelnewsasia.com' in site:\n",
    "            # Headline extraction\n",
    "            headlines = []\n",
    "            h1_elements = soup.select('h1.layout__title, h1.h1--page-title, div.article-header h1')\n",
    "            if h1_elements:\n",
    "                headlines.append(h1_elements[0].get_text().strip())\n",
    "            \n",
    "            # Define termination markers and AI disclaimers\n",
    "            termination_markers = [\n",
    "                \"Subscribe to our Chief Editor's Week in Review\",\n",
    "                \"Get our pick of top stories and thought-provoking articles in your inbox\"\n",
    "            ]\n",
    "            ai_disclaimers = [\"This audio is generated by an AI tool\"]\n",
    "            \n",
    "            # Define sections to exclude\n",
    "            excluded_sections = [\n",
    "                'div.referenced-card',    # \"Related:\" section with links to other articles\n",
    "                '.elementor-grid-item',   # Grid items for other articles\n",
    "                '.media-object',          # Media object containers (often related articles)\n",
    "                '.teaser',                # Article teasers\n",
    "                'div.recommended-articles', # Recommended articles section\n",
    "                '.read-more',             # \"Read more\" sections\n",
    "                '.article-tags',          # Article tags\n",
    "                '.article-social-share',  # Social share buttons\n",
    "                '[data-title=\"Related\"]'  # Related content sections\n",
    "            ]\n",
    "    \n",
    "            # Helper function to check if element should be excluded\n",
    "            def should_exclude(element):\n",
    "                # Check if the element itself has an excluded class\n",
    "                if element.get('class'):\n",
    "                    element_classes = ' '.join(element.get('class'))\n",
    "                    for selector in excluded_sections:\n",
    "                        class_name = selector.replace('.', '')\n",
    "                        if class_name in element_classes:\n",
    "                            return True\n",
    "                \n",
    "                # Check parent elements\n",
    "                parent = element.parent\n",
    "                while parent and parent.name != 'body':\n",
    "                    if parent.get('class'):\n",
    "                        parent_classes = ' '.join(parent.get('class'))\n",
    "                        for selector in excluded_sections:\n",
    "                            class_name = selector.replace('.', '')\n",
    "                            if class_name in parent_classes:\n",
    "                                return True\n",
    "                    \n",
    "                    # Check for data attributes\n",
    "                    if parent.get('data-title') == 'Related':\n",
    "                        return True\n",
    "                        \n",
    "                    parent = parent.parent\n",
    "                    \n",
    "                return False\n",
    "    \n",
    "            # Function to check if text should be skipped or signals termination\n",
    "            def should_skip_or_terminate(text):\n",
    "                if any(marker in text for marker in termination_markers):\n",
    "                    logger.info(\"Found newsletter subscription marker - terminating extraction\")\n",
    "                    return \"terminate\"\n",
    "                elif any(disclaimer in text for disclaimer in ai_disclaimers):\n",
    "                    return \"skip\"\n",
    "                return False\n",
    "            \n",
    "            # Add the main headline first\n",
    "            structured_content = headlines.copy()\n",
    "            \n",
    "            # Find the main content container - more likely to have the correct element ordering\n",
    "            main_content = None\n",
    "            content_selectors = [\n",
    "                'section[data-title=\"Content\"]',\n",
    "                'div.layout_region',\n",
    "                'div.article-body',\n",
    "                'div.article-content',\n",
    "                'div.content-wrapper'\n",
    "            ]\n",
    "            \n",
    "            for selector in content_selectors:\n",
    "                container = soup.select_one(selector)\n",
    "                if container:\n",
    "                    main_content = container\n",
    "                    break\n",
    "    \n",
    "            # If we found the main content, process all elements in document order\n",
    "            if main_content:\n",
    "                # Get ALL elements in their natural document order\n",
    "                all_elements = main_content.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'ul', 'ol', 'div'])\n",
    "                \n",
    "                # Process elements in the order they appear in the document\n",
    "                terminate_extraction = False\n",
    "                \n",
    "                for element in all_elements:\n",
    "                    if terminate_extraction:\n",
    "                        break\n",
    "                        \n",
    "                    # Skip elements in excluded sections\n",
    "                    if should_exclude(element):\n",
    "                        continue\n",
    "                    \n",
    "                    # Process headers\n",
    "                    if element.name.startswith('h') and element.name != 'h1':\n",
    "                        header_text = element.get_text().strip()\n",
    "                        if header_text and len(header_text) > 1:\n",
    "                            # Add header as a markdown section\n",
    "                            structured_content.append(f\"## {header_text}\")\n",
    "                    \n",
    "                    # Process paragraphs\n",
    "                    elif element.name == 'p':\n",
    "                        p_text = element.get_text().strip()\n",
    "                        \n",
    "                        # Skip empty paragraphs or very short ones\n",
    "                        if not p_text or len(p_text) <= 1:\n",
    "                            continue\n",
    "                    \n",
    "                        # Check for termination markers\n",
    "                        check_result = should_skip_or_terminate(p_text)\n",
    "                        if check_result == \"terminate\":\n",
    "                            terminate_extraction = True\n",
    "                            break\n",
    "                        elif check_result == \"skip\":\n",
    "                            continue\n",
    "                        \n",
    "                        # Add the paragraph text\n",
    "                        structured_content.append(p_text)\n",
    "                    \n",
    "                    # Process lists\n",
    "                    elif element.name in ['ul', 'ol']:\n",
    "                        list_items = element.find_all('li')\n",
    "                        for item in list_items:\n",
    "                            item_text = item.get_text().strip()\n",
    "                            if item_text:\n",
    "                                structured_content.append(f\"• {item_text}\")\n",
    "                    \n",
    "                    # Process divs that might contain text (but only if they have text-long class)\n",
    "                    elif element.name == 'div' and element.get('class') and 'text-long' in element.get('class'):\n",
    "                        # Only process if direct text (not through child elements we'll process separately)\n",
    "                        direct_text = ''.join([text for text in element.contents if isinstance(text, str)]).strip()\n",
    "                        if direct_text and len(direct_text) > 1:\n",
    "                            structured_content.append(direct_text)\n",
    "            else:\n",
    "                # Fallback to old approach if main content container not found\n",
    "                logger.warning(\"Main content container not found, using fallback approach\")\n",
    "                \n",
    "                # This is a simplified version of the old approach\n",
    "                paragraphs = soup.find_all('p')\n",
    "                for p in paragraphs:\n",
    "                    if should_exclude(p):\n",
    "                        continue\n",
    "                        \n",
    "                    p_text = p.get_text().strip()\n",
    "                    if p_text and len(p_text) > 1:\n",
    "                        structured_content.append(p_text)\n",
    "            \n",
    "            # Deduplicate content\n",
    "            deduplicated_content = []\n",
    "            seen_content = set()\n",
    "            \n",
    "            for item in structured_content:\n",
    "                # Normalize whitespace for comparison\n",
    "                item_normalized = re.sub(r'\\s+', ' ', item).strip()\n",
    "                \n",
    "                # Skip if we've seen this exact content\n",
    "                if item_normalized in seen_content:\n",
    "                    continue\n",
    "                \n",
    "                deduplicated_content.append(item)\n",
    "                seen_content.add(item_normalized)\n",
    "            \n",
    "            # Replace the original list with deduplicated content\n",
    "            structured_content = deduplicated_content\n",
    "    \n",
    "            # Log extraction results\n",
    "            logger.debug(f\"CNA Article Extraction: Found {len(structured_content)} content items\")\n",
    "\n",
    "########$#######################################################################################        \n",
    "        # Add this to your extract_journalist method\n",
    "        \n",
    "        # CNN journalist extraction\n",
    "        elif 'cnn.com' in site or 'edition.cnn.com' in site:\n",
    "            # Method 1: Look for byline section containing journalist names\n",
    "            byline_selectors = [\n",
    "                'div[data-component-name=\"byline\"]',\n",
    "                'div.byline',\n",
    "                'div.byline_names',\n",
    "                'div.headline__byline',\n",
    "                'div[class*=\"byline\"]'\n",
    "            ]\n",
    "            \n",
    "            for selector in byline_selectors:\n",
    "                byline_elements = soup.select(selector)\n",
    "                if byline_elements:\n",
    "                    for element in byline_elements:\n",
    "                        # Extract all text from byline\n",
    "                        byline_text = element.get_text().strip()\n",
    "                        \n",
    "                        # Clean up byline text\n",
    "                        byline_text = byline_text.replace(\"By \", \"\").strip()\n",
    "                        if byline_text and len(byline_text) > 3:\n",
    "                            return byline_text\n",
    "    \n",
    "            # Method 2: Look for specific byline elements (like those in screenshots)\n",
    "            byline_name_selectors = [\n",
    "                'span.byline__name',\n",
    "                'span.byline_name',\n",
    "                'a[class*=\"byline_link\"]',\n",
    "                'div[class*=\"byline-sub-text\"]'\n",
    "            ]\n",
    "            \n",
    "            all_names = []\n",
    "            for selector in byline_name_selectors:\n",
    "                name_elements = soup.select(selector)\n",
    "                for element in name_elements:\n",
    "                    name = element.get_text().strip()\n",
    "                    if name and len(name) > 3 and name not in all_names:\n",
    "                        all_names.append(name)\n",
    "            \n",
    "            if all_names:\n",
    "                return \", \".join(all_names)\n",
    "            \n",
    "            # Method 3: Look for common CNN byline formats\n",
    "            # Pattern: \"By [Author Name], CNN\"\n",
    "            byline_patterns = [\n",
    "                r'By\\s+([\\w\\s]+),\\s+CNN',\n",
    "                r'By\\s+([\\w\\s]+)\\s+and\\s+([\\w\\s]+),\\s+CNN',\n",
    "                r'By\\s+([\\w\\s]+),\\s+([\\w\\s]+)\\s+and\\s+([\\w\\s]+),\\s+CNN'\n",
    "            ]\n",
    "            \n",
    "            page_text = soup.get_text()\n",
    "            for pattern in byline_patterns:\n",
    "                match = re.search(pattern, page_text)\n",
    "                if match:\n",
    "                    # Join all captured groups\n",
    "                    return \", \".join(match.groups())\n",
    "    \n",
    "            # Method 4: Extract from byline text in paragraphs\n",
    "            for p in soup.find_all('p'):\n",
    "                text = p.get_text().strip()\n",
    "                if text.startswith('By ') and ', CNN' in text and len(text) < 100:\n",
    "                    return text.replace('By ', '').replace(', CNN', '').strip()\n",
    "                    \n",
    "            # Method 5: Check for CNN staff/service attribution\n",
    "            for p in soup.find_all('p')[-3:]:  # Check last few paragraphs\n",
    "                text = p.get_text().strip()\n",
    "                if 'CNN' in text and any(x in text.lower() for x in ['contributed', 'reporting', 'service']):\n",
    "                    # Extract just the CNN attribution\n",
    "                    return \"CNN\"\n",
    "                    \n",
    "            # Default to CNN if we found nothing else\n",
    "            return \"CNN\"\n",
    "\n",
    "########$#######################################################################################\n",
    "\n",
    "    def generate_filename(self, article):\n",
    "        \"\"\"\n",
    "        Generate a filename for an article based on the naming convention\n",
    "        \n",
    "        Args:\n",
    "            article (dict): Article metadata dictionary\n",
    "        \n",
    "        Returns:\n",
    "            str: Generated filename\n",
    "        \"\"\"\n",
    "        # Get date in YYYYMMDD format\n",
    "        try:\n",
    "            date_obj = pd.to_datetime(article.get('Date'))\n",
    "            date_str = date_obj.strftime('%Y%m%d')\n",
    "        except:\n",
    "            date_str = '00000000'\n",
    "        \n",
    "        # Get source \n",
    "        source = article.get('Media', self.source)\n",
    "        source = str(source).replace('The ', '').strip().split()[0]\n",
    "        \n",
    "        # Get topic\n",
    "        topic = article.get('Theme', self.topic)\n",
    "        if isinstance(topic, str):\n",
    "            # Standardize topic names\n",
    "            topic_mapping = {\n",
    "                'Cybercrime': 'Cybercrime',\n",
    "                'Forensic': 'Forensic',\n",
    "                'Misinformation': 'Misinformation',\n",
    "                'Medical Fraud': 'MedicalFraud',\n",
    "                'Organised Crime': 'OrganisedCrime'\n",
    "            }\n",
    "            topic = next((mapped for key, mapped in topic_mapping.items() if key in topic), topic)\n",
    "        \n",
    "        # Get relevance score\n",
    "        relevance = article.get('Relevance', 0)\n",
    "        \n",
    "        # Clean title for filename\n",
    "        title = self.clean_title_for_filename(article.get('Title', ''))\n",
    "        \n",
    "        # Generate filename\n",
    "        filename = f\"{date_str}_{source}{topic}R{relevance}_{title}.txt\"\n",
    "        \n",
    "        return filename\n",
    "    \n",
    "    def clean_title_for_filename(self, title):\n",
    "        \"\"\"\n",
    "        Clean a title for use in a filename\n",
    "        \n",
    "        Args:\n",
    "            title (str): Original title\n",
    "        \n",
    "        Returns:\n",
    "            str: Cleaned title suitable for filename\n",
    "        \"\"\"\n",
    "        if not title:\n",
    "            return \"no-title\"\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        title = title.lower()\n",
    "        # Replace spaces with hyphens\n",
    "        title = title.replace(' ', '-')\n",
    "        # Remove special characters\n",
    "        title = re.sub(r'[^a-z0-9\\-]', '-', title)\n",
    "        # Replace multiple hyphens with single hyphen\n",
    "        title = re.sub(r'\\-+', '-', title)\n",
    "        # Truncate to 50 characters\n",
    "        title = title[:50].rstrip('-')\n",
    "        \n",
    "        return title\n",
    "    \n",
    "    def clean_date_format(self, date_value):\n",
    "        \"\"\"\n",
    "        Clean date format to remove time component and ensure DD/MM/YYYY format\n",
    "        \n",
    "        Args:\n",
    "            date_value: Input date value\n",
    "        \n",
    "        Returns:\n",
    "            str: Formatted date string\n",
    "        \"\"\"\n",
    "        if pd.isna(date_value):\n",
    "            return date_value\n",
    "        \n",
    "        try:\n",
    "            # If it's already a datetime object\n",
    "            if isinstance(date_value, datetime):\n",
    "                return date_value.strftime('%d/%m/%Y')\n",
    "            \n",
    "            # If it's a string\n",
    "            if isinstance(date_value, str):\n",
    "                # Try to extract date part before any time indicators\n",
    "                date_part = date_value\n",
    "                if 'T' in date_part:\n",
    "                    date_part = date_part.split('T')[0]\n",
    "                elif ' ' in date_part:\n",
    "                    date_part = date_part.split(' ')[0]\n",
    "                \n",
    "                # Try to parse and reformat\n",
    "                try:\n",
    "                    date_obj = pd.to_datetime(date_part)\n",
    "                    return date_obj.strftime('%d/%m/%Y')\n",
    "                except:\n",
    "                    return date_part\n",
    "            \n",
    "            # Return as is if we can't process it\n",
    "            return date_value\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error cleaning date format: {str(e)}\")\n",
    "            return date_value\n",
    "\n",
    "    def extract_and_save_article(self, article):\n",
    "        \"\"\"\n",
    "        Extract the article from the URL and save it to the appropriate directories\n",
    "        \"\"\"\n",
    "        url = article.get('URL', '')\n",
    "        is_paywall = article.get('Paywall', 'N') == 'Y'\n",
    "        \n",
    "        if not url:\n",
    "            logger.warning(f\"No URL for article with title: {article.get('Title')}\")\n",
    "            return False, None, None\n",
    "        \n",
    "        try:\n",
    "            # Add random delay to avoid being blocked\n",
    "            time.sleep(random.uniform(1, 3))\n",
    "            \n",
    "            # For paywalled articles, always use archive buttons URL\n",
    "            if is_paywall:\n",
    "                archive_url = f\"https://www.archivebuttons.com/articles?article={url}\"\n",
    "                logger.info(f\"Processing paywalled article via archive: {archive_url}\")\n",
    "                \n",
    "                # Additional wait for archive buttons to load\n",
    "                time.sleep(random.uniform(3, 5))\n",
    "                \n",
    "                # Use archive buttons URL for request\n",
    "                current_url = archive_url\n",
    "            else:\n",
    "                # Use original URL for non-paywalled articles\n",
    "                current_url = url\n",
    "            \n",
    "            # Make the request\n",
    "            response = requests.get(current_url, headers=self.headers, timeout=15)\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                logger.error(f\"Failed to retrieve URL: HTTP {response.status_code}\")\n",
    "                return False, None, None\n",
    "            \n",
    "            # Parse the HTML content\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Extract journalist name\n",
    "            journalist = article.get('Journalist')\n",
    "            if not journalist or pd.isna(journalist) or str(journalist).strip() == '':\n",
    "                journalist = self.extract_journalist(soup, url)\n",
    "            \n",
    "            # Extract article text\n",
    "            article_text = self.extract_article_text(soup, url)\n",
    "            \n",
    "            if not article_text:\n",
    "                logger.error(f\"Could not extract article text from {current_url}\")\n",
    "                return False, journalist, None\n",
    "            \n",
    "            # Generate filename\n",
    "            filename = self.generate_filename(article)\n",
    "            \n",
    "            # Format the date\n",
    "            formatted_date = self.clean_date_format(article.get('Date', ''))\n",
    "            \n",
    "            # Create a header with metadata\n",
    "            header = f\"\"\"Title: {article.get('Title', 'No Title')}\n",
    "Date: {formatted_date}\n",
    "Source: {article.get('Media', self.source)}\n",
    "Topic: {article.get('Theme', self.topic)}\n",
    "Category: {article.get('Category', '')}\n",
    "Relevance: {article.get('Relevance', '')}\n",
    "URL: {url}\n",
    "Paywall: {'Y' if is_paywall else 'N'}\n",
    "\"\"\"\n",
    "            if journalist:\n",
    "                header += f\"Journalist: {journalist}\\n\"\n",
    "        \n",
    "            # Combine header and article text\n",
    "            content = f\"{header}\\n\\n{article_text}\"\n",
    "            \n",
    "            # Save to main file by source\n",
    "            source_path = f\"{self.output_dir}/by_source/{self.source}/{filename}\"\n",
    "            with open(source_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(content)\n",
    "            \n",
    "            # Save to topic directory\n",
    "            topic_path = f\"{self.output_dir}/by_topic/{self.topic}/{filename}\"\n",
    "            with open(topic_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(content)\n",
    "            \n",
    "            # Save to relevance directory\n",
    "            relevance = article.get('Relevance', 0)\n",
    "            relevance_suffix = {\n",
    "                5: 'Most_',\n",
    "                4: 'Highly_',\n",
    "                3: '',\n",
    "                2: 'Somewhat_',\n",
    "                1: 'Least_'\n",
    "            }.get(relevance, '')\n",
    "            \n",
    "            relevance_path = f\"{self.output_dir}/by_relevance/R{relevance}_{relevance_suffix}Relevant/{filename}\"\n",
    "            with open(relevance_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(content)\n",
    "            \n",
    "            logger.info(f\"Successfully saved article: {filename}\")\n",
    "            return True, journalist, filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting article from {url}: {str(e)}\", exc_info=True)\n",
    "            return False, None, None\n",
    "    \n",
    "    def check_paywall(self, soup):\n",
    "        \"\"\"\n",
    "        Detect if the article is paywalled        \n",
    "        Args:\n",
    "            soup (BeautifulSoup): Parsed HTML content        \n",
    "        Returns:\n",
    "            bool: True if article is paywalled, False otherwise\n",
    "        \"\"\"\n",
    "        # Look for the specific div with \"FOR SUBSCRIBERS\" text\n",
    "        paywall_div = soup.find('div', class_=lambda x: x and 'border-t-5' in x and 'border-solid' in x and 'uppercase' in x)\n",
    "        \n",
    "        if paywall_div:\n",
    "            paywall_text = paywall_div.get_text(strip=True).upper()\n",
    "            return paywall_text == 'FOR SUBSCRIBERS'\n",
    "        \n",
    "        return False    \n",
    "\n",
    "    def process_excel_file(self):\n",
    "        try:\n",
    "            # Read the Excel file\n",
    "            logger.info(f\"Reading Excel file: {self.excel_file}\")\n",
    "            df = pd.read_excel(self.excel_file)\n",
    "        \n",
    "            # Add Paywall column if it doesn't exist\n",
    "            if 'Paywall' not in df.columns:\n",
    "                df['Paywall'] = 'N'\n",
    "            \n",
    "            # Clean up date format in the DataFrame\n",
    "            if 'Date' in df.columns:\n",
    "                logger.info(\"Cleaning date format in Excel file\")\n",
    "                df['Date'] = df['Date'].apply(self.clean_date_format)\n",
    "            \n",
    "            # Add filename column if it doesn't exist\n",
    "            if 'Filename' not in df.columns:\n",
    "                df['Filename'] = None\n",
    "            \n",
    "            # Create a copy to track updates\n",
    "            updated_df = df.copy()\n",
    "            \n",
    "            # Process each article\n",
    "            for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Extracting articles\"):\n",
    "                # Convert row to dict for easier handling\n",
    "                article = row.to_dict()\n",
    "                \n",
    "                # Skip if already processed and successful\n",
    "                if not pd.isna(updated_df.at[index, 'Filename']):\n",
    "                    logger.info(f\"Skipping already processed article: {article.get('Title')}\")\n",
    "                    continue\n",
    "                \n",
    "                # Original URL\n",
    "                original_url = article.get('URL', '')\n",
    "                    \n",
    "                try:\n",
    "                    # Fetch the page to check paywall status\n",
    "                    response = requests.get(original_url, headers=self.headers, timeout=10)\n",
    "                    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                    \n",
    "                    # Check for \"FOR SUBSCRIBERS\" text\n",
    "                    paywall_div = soup.find('div', class_=lambda x: x and 'border-t-5' in x and 'border-solid' in x and 'uppercase' in x)\n",
    "                    is_paywall = (paywall_div and 'FOR SUBSCRIBERS' in paywall_div.get_text(strip=True).upper())\n",
    "                    \n",
    "                    # Update Paywall column\n",
    "                    updated_df.at[index, 'Paywall'] = 'Y' if is_paywall else 'N'\n",
    "                    \n",
    "                    # Update the article dictionary with paywall status\n",
    "                    article['Paywall'] = 'Y' if is_paywall else 'N'\n",
    "                    \n",
    "                    # Extract and save the article\n",
    "                    success, journalist, filename = self.extract_and_save_article(article)\n",
    "                    \n",
    "                    # Update the DataFrame with filename\n",
    "                    if success:\n",
    "                        updated_df.at[index, 'Filename'] = filename\n",
    "                        \n",
    "                        # If Journalist column exists and is empty, update it with extracted journalist\n",
    "                        if 'Journalist' in updated_df.columns and (\n",
    "                            pd.isna(updated_df.at[index, 'Journalist']) or \n",
    "                            updated_df.at[index, 'Journalist'] is None or \n",
    "                            str(updated_df.at[index, 'Journalist']).strip() == ''\n",
    "                        ):\n",
    "                            if journalist:\n",
    "                                updated_df.at[index, 'Journalist'] = journalist\n",
    "            \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing article {original_url}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Save the updated Excel file\n",
    "            output_excel = self.excel_file.replace('.xlsx', 'Updated.xlsx')\n",
    "            logger.info(f\"Saving updated Excel file: {output_excel}\")\n",
    "            \n",
    "            # Convert dates to string format before saving\n",
    "            if 'Date' in updated_df.columns:\n",
    "                updated_df['Date'] = updated_df['Date'].apply(\n",
    "                    lambda x: self.clean_date_format(x) if not pd.isna(x) else x\n",
    "                )\n",
    "            \n",
    "            # Save the updated Excel file\n",
    "            updated_df.to_excel(output_excel, index=False)\n",
    "            logger.info(f\"Successfully saved to {output_excel}\")\n",
    "            \n",
    "            return updated_df\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing Excel file: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Run the entire extraction process\n",
    "        Returns:\n",
    "            bool: True if extraction was successful, False otherwise\n",
    "        \"\"\"\n",
    "        logger.info(f\"Starting extraction from {self.excel_file}\")\n",
    "        logger.info(f\"Source: {self.source}, Topic: {self.topic}\")\n",
    "        \n",
    "        try:\n",
    "            updated_df = self.process_excel_file()\n",
    "            \n",
    "            # Print summary\n",
    "            total = len(updated_df)\n",
    "            extracted = len(updated_df[~updated_df['Filename'].isna()])\n",
    "            \n",
    "            logger.info(f\"Extraction complete: {extracted}/{total} articles extracted\")\n",
    "            logger.info(f\"Articles saved to {self.output_dir}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Extraction failed: {str(e)}\")\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2bb73e72-3ba8-4865-a4ba-613a5107df24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the article extraction process\n",
    "    \"\"\"\n",
    "    import sys\n",
    "    \n",
    "    input_file = \"CNN_Organised_Crime_Articles_Processed.xlsx\"\n",
    "    \n",
    "    try:\n",
    "        # Create and run the extractor\n",
    "        extractor = NewsArticleExtractor(input_file)\n",
    "        success = extractor.run()\n",
    "        \n",
    "        # Exit with appropriate status code\n",
    "        sys.exit(0 if success else 1)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fatal error: {str(e)}\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7b3b3bd6-bcd5-4031-9829-b568210de884",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17 21:04:22,451 - INFO - Starting extraction from CNN_Organised_Crime_Articles_Processed.xlsx\n",
      "2025-03-17 21:04:22,452 - INFO - Source: CNN, Topic: Organised_Crime\n",
      "2025-03-17 21:04:22,453 - INFO - Reading Excel file: CNN_Organised_Crime_Articles_Processed.xlsx\n",
      "2025-03-17 21:04:22,536 - INFO - Cleaning date format in Excel file\n",
      "Extracting articles:   0%|          | 0/100 [00:00<?, ?it/s]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '28/03/2024' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '28/03/2024' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:04:55,806 - INFO - Successfully saved article: 20240328_CNNOrganisedCrimeR5_gangs-netting-up-to-3-trillion-a-year-as-southeast.txt\n",
      "Extracting articles:   1%|          | 1/100 [00:33<54:53, 33.27s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '29/09/2024' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '29/09/2024' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:05:29,529 - INFO - Successfully saved article: 20240929_CNNOrganisedCrimeR5_-this-is-not-luck-this-is-a-systemic-approach-thes.txt\n",
      "Extracting articles:   2%|▏         | 2/100 [01:06<54:46, 33.53s/it]2025-03-17 21:06:03,423 - INFO - Successfully saved article: 20230310_CNNOrganisedCrimeR5_27-suspected-la-gang-members-arrested-in-connectio.txt\n",
      "Extracting articles:   3%|▎         | 3/100 [01:40<54:28, 33.70s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '22/02/2025' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '22/02/2025' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:06:35,801 - INFO - Successfully saved article: 20250222_CNNOrganisedCrimeR5_the-us-now-considers-these-cartels-and-gangs-terro.txt\n",
      "Extracting articles:   4%|▍         | 4/100 [02:13<53:05, 33.18s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '18/09/2024' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '18/09/2024' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:07:09,037 - INFO - Successfully saved article: 20240918_CNNOrganisedCrimeR5_what-is-racketeering-the-crime-explained.txt\n",
      "Extracting articles:   5%|▌         | 5/100 [02:46<52:33, 33.20s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '24/05/2024' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '24/05/2024' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:07:13,339 - INFO - Successfully saved article: 20240524_CNNOrganisedCrimeR5_retailers-use-police-like-investigation-centers-to.txt\n",
      "Extracting articles:   6%|▌         | 6/100 [02:50<36:37, 23.37s/it]2025-03-17 21:07:46,034 - INFO - Successfully saved article: 20241107_CNNOrganisedCrimeR5_biden-administration-designates-dangerous-venezuel.txt\n",
      "Extracting articles:   7%|▋         | 7/100 [03:23<40:57, 26.42s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '22/08/2022' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '22/08/2022' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:08:16,526 - INFO - Successfully saved article: 20220822_CNNOrganisedCrimeR5_this-country-calls-time-on-the-war-on-drugs.txt\n",
      "Extracting articles:   8%|▊         | 8/100 [03:53<42:29, 27.72s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '27/07/2024' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '27/07/2024' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:08:48,820 - INFO - Successfully saved article: 20240727_CNNOrganisedCrimeR5_the-alleged-mexican-drug-cartel-bosses-arrested-or.txt\n",
      "Extracting articles:   9%|▉         | 9/100 [04:26<44:12, 29.15s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '23/02/2023' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '23/02/2023' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:09:19,601 - INFO - Successfully saved article: 20230223_CNNOrganisedCrimeR5_biden-administration-sanctions-mexican-drug-cartel.txt\n",
      "Extracting articles:  10%|█         | 10/100 [04:57<44:28, 29.65s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '13/09/2024' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '13/09/2024' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:09:51,528 - INFO - Successfully saved article: 20240913_APOrganisedCrimeR5_longtime-mexican-drug-cartel-leader-pleads-not-gui.txt\n",
      "Extracting articles:  11%|█         | 11/100 [05:28<45:01, 30.35s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '19/06/2024' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '19/06/2024' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:10:25,470 - INFO - Successfully saved article: 20240619_CNNOrganisedCrimeR5_us-announces-major-bust-as-efforts-to-crack-down-o.txt\n",
      "Extracting articles:  12%|█▏        | 12/100 [06:02<46:06, 31.44s/it]2025-03-17 21:10:58,415 - INFO - Successfully saved article: 20240412_CNNOrganisedCrimeR5_uk-us-clamp-down-on-money-laundering-rings-alleged.txt\n",
      "Extracting articles:  13%|█▎        | 13/100 [06:35<46:15, 31.90s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '28/07/2024' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '28/07/2024' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:11:31,930 - INFO - Successfully saved article: 20240728_CNNOrganisedCrimeR5_what-we-know-about-the-sinaloa-cartel-and-its-lead.txt\n",
      "Extracting articles:  14%|█▍        | 14/100 [07:09<46:25, 32.39s/it]2025-03-17 21:12:08,024 - INFO - Successfully saved article: 20230305_CNNOrganisedCrimeR5_over-130-arrested-in-anti-mafia-raids-across-multi.txt\n",
      "Extracting articles:  15%|█▌        | 15/100 [07:45<47:27, 33.50s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '27/10/2021' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '27/10/2021' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:12:41,732 - INFO - Successfully saved article: 20211027_CNNOrganisedCrimeR5_justice-department-announces-150-arrests-in-operat.txt\n",
      "Extracting articles:  16%|█▌        | 16/100 [08:19<46:59, 33.57s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '29/07/2024' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '29/07/2024' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:13:14,433 - INFO - Successfully saved article: 20240729_CNNOrganisedCrimeR5_one-of-the-biggest-cartel-busts-in-recent-memory-b.txt\n",
      "Extracting articles:  17%|█▋        | 17/100 [08:51<46:04, 33.31s/it]2025-03-17 21:13:49,405 - INFO - Successfully saved article: 20230610_CNNOrganisedCrimeR5_prolific-cybercrime-group-reemerges-following-fbi.txt\n",
      "Extracting articles:  18%|█▊        | 18/100 [09:26<46:12, 33.81s/it]2025-03-17 21:14:23,181 - INFO - Successfully saved article: 20230403_ReutersOrganisedCrimeR5_international-drug-bust-nets-677-million-of-cocain.txt\n",
      "Extracting articles:  19%|█▉        | 19/100 [10:00<45:37, 33.80s/it]2025-03-17 21:14:55,887 - INFO - Successfully saved article: 20230508_CNNOrganisedCrimeR5_-this-isn-t-some-random-dude-with-a-duffel-bag-to.txt\n",
      "Extracting articles:  20%|██        | 20/100 [10:33<44:37, 33.47s/it]2025-03-17 21:15:26,075 - INFO - Successfully saved article: 20230908_CNNOrganisedCrimeR5_colombian-drug-lord-otoniel-sentenced-to-45-years.txt\n",
      "Extracting articles:  21%|██        | 21/100 [11:03<42:46, 32.48s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '26/07/2023' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '26/07/2023' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:15:58,697 - INFO - Successfully saved article: 20230726_CNNOrganisedCrimeR5_how-gang-warfare-jumped-from-ecuador-s-prisons-to.txt\n",
      "Extracting articles:  22%|██▏       | 22/100 [11:36<42:17, 32.53s/it]2025-03-17 21:16:34,511 - INFO - Successfully saved article: 20241002_CNNOrganisedCrimeR5_us-cracks-down-on-hacking-network-with-thousands-o.txt\n",
      "Extracting articles:  23%|██▎       | 23/100 [12:11<43:00, 33.51s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '31/05/2022' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '31/05/2022' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:17:07,607 - INFO - Successfully saved article: 20220531_CNNOrganisedCrimeR5_1-billion-meth-pills-seized-as-asia-sees-record-dr.txt\n",
      "Extracting articles:  24%|██▍       | 24/100 [12:45<42:17, 33.39s/it]2025-03-17 21:17:40,655 - INFO - Successfully saved article: 20240903_CNNOrganisedCrimeR5_former-honduran-president-found-guilty-in-drug-tra.txt\n",
      "Extracting articles:  25%|██▌       | 25/100 [13:18<41:36, 33.29s/it]2025-03-17 21:18:14,297 - INFO - Successfully saved article: 20230303_CNNOrganisedCrimeR5_biden-administration-sanctions-what-it-says-is-a-t.txt\n",
      "Extracting articles:  26%|██▌       | 26/100 [13:51<41:11, 33.39s/it]2025-03-17 21:18:45,432 - INFO - Successfully saved article: 20241104_CNNOrganisedCrimeR4_mexican-drug-gangs-increasingly-targeting-australi.txt\n",
      "Extracting articles:  27%|██▋       | 27/100 [14:22<39:48, 32.72s/it]2025-03-17 21:19:17,516 - INFO - Successfully saved article: 20230206_CNNOrganisedCrimeR4_with-no-more-covid-restrictions-asia-s-drug-cartel.txt\n",
      "Extracting articles:  28%|██▊       | 28/100 [14:54<39:01, 32.53s/it]2025-03-17 21:19:51,002 - INFO - Successfully saved article: 20240310_CNNOrganisedCrimeR3_dozens-of-suspected-white-supremacist-gang-members.txt\n",
      "Extracting articles:  29%|██▉       | 29/100 [15:28<38:49, 32.81s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '21/09/2023' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '21/09/2023' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:20:25,655 - INFO - Successfully saved article: 20230921_CNNOrganisedCrimeR3_suspects-in-bronx-day-care-overdose-cases-that-lef.txt\n",
      "Extracting articles:  30%|███       | 30/100 [16:03<38:55, 33.37s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '15/05/2024' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '15/05/2024' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:21:00,704 - INFO - Successfully saved article: 20240515_CNNOrganisedCrimeR3_in-a-city-cut-off-from-the-world-guns-and-drugs-ke.txt\n",
      "Extracting articles:  31%|███       | 31/100 [16:38<38:57, 33.87s/it]2025-03-17 21:21:06,140 - INFO - Successfully saved article: 20230807_CNNOrganisedCrimeR3_it-s-not-only-ny-la-san-francisco-retail-crime-has.txt\n",
      "Extracting articles:  32%|███▏      | 32/100 [16:43<28:43, 25.34s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '22/07/2021' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '22/07/2021' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:21:41,769 - INFO - Successfully saved article: 20210722_CNNOrganisedCrimeR2_doj-creates-gun-trafficking-strike-forces-in-five.txt\n",
      "Extracting articles:  33%|███▎      | 33/100 [17:19<31:44, 28.43s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '17/10/2020' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '17/10/2020' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:22:15,043 - INFO - Successfully saved article: 20201017_CNNOrganisedCrimeR2_prosecutors-in-utah-file-drug-trafficking-charges.txt\n",
      "Extracting articles:  34%|███▍      | 34/100 [17:52<32:52, 29.88s/it]2025-03-17 21:22:50,338 - INFO - Successfully saved article: 20240328_CNNOrganisedCrimeR2_-i-knew-i-could-be-killed-this-cash-van-guard-is-j.txt\n",
      "Extracting articles:  35%|███▌      | 35/100 [18:27<34:07, 31.51s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '21/10/2024' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '21/10/2024' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:22:55,019 - INFO - Successfully saved article: 20241021_CNNOrganisedCrimeR2_exclusive-inside-the-prison-that-executes-people-f.txt\n",
      "Extracting articles:  36%|███▌      | 36/100 [18:32<25:01, 23.46s/it]2025-03-17 21:23:29,720 - INFO - Successfully saved article: 20230809_CNNOrganisedCrimeR2_21-people-indicted-and-90-dogs-seized-in-indiana-d.txt\n",
      "Extracting articles:  37%|███▋      | 37/100 [19:07<28:10, 26.83s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '21/07/2024' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '21/07/2024' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:24:03,483 - INFO - Successfully saved article: 20240721_CNNOrganisedCrimeR2_south-african-police-discover-multimillion-dollar.txt\n",
      "Extracting articles:  38%|███▊      | 38/100 [19:40<29:52, 28.91s/it]2025-03-17 21:24:35,258 - INFO - Successfully saved article: 20250103_CNNOrganisedCrimeR2_dozens-arrested-in-global-operation-over-ai-genera.txt\n",
      "Extracting articles:  39%|███▉      | 39/100 [20:12<30:15, 29.77s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '20/09/2023' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '20/09/2023' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:25:09,977 - INFO - Successfully saved article: 20230920_CNNOrganisedCrimeR2_us-soldiers-face-south-korea-drug-probe-after-poli.txt\n",
      "Extracting articles:  40%|████      | 40/100 [20:47<31:15, 31.25s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '15/10/2022' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '15/10/2022' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:25:43,581 - INFO - Successfully saved article: 20221015_CNNOrganisedCrimeR2_-rape-has-become-a-weapon-for-haiti-gangs-says-un.txt\n",
      "Extracting articles:  41%|████      | 41/100 [21:21<31:25, 31.96s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '20/07/2022' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '20/07/2022' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:26:15,472 - INFO - Successfully saved article: 20220720_CNNOrganisedCrimeR2_-law-order-organized-crime-crew-member-fatally-sho.txt\n",
      "Extracting articles:  42%|████▏     | 42/100 [21:52<30:52, 31.94s/it]2025-03-17 21:26:25,879 - ERROR - Error processing article https://www.cnn.com/2021/11/30/us/smash-grab-crimes-organized-retail-thefts/index.html: HTTPSConnectionPool(host='edition.cnn.com', port=443): Read timed out. (read timeout=10)\n",
      "Extracting articles:  43%|████▎     | 43/100 [22:03<24:12, 25.48s/it]2025-03-17 21:26:57,874 - INFO - Successfully saved article: 20240507_CNNOrganisedCrimeR2_russia-sentences-us-citizen-to-12-years-in-penal-c.txt\n",
      "Extracting articles:  44%|████▍     | 44/100 [22:35<25:36, 27.43s/it]2025-03-17 21:27:32,146 - INFO - Successfully saved article: 20250403_CNNOrganisedCrimeR2_four-people-charged-with-operating-one-of-the-larg.txt\n",
      "Extracting articles:  45%|████▌     | 45/100 [23:09<27:01, 29.49s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '15/11/2022' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '15/11/2022' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:28:06,025 - INFO - Successfully saved article: 20221115_CNNOrganisedCrimeR1_retired-detective-and-kansas-city-drug-kingpin-cha.txt\n",
      "Extracting articles:  46%|████▌     | 46/100 [23:43<27:43, 30.80s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '16/06/2023' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '16/06/2023' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:28:41,282 - INFO - Successfully saved article: 20230616_CNNOrganisedCrimeR1_opinion-rethinking-human-trafficking.txt\n",
      "Extracting articles:  47%|████▋     | 47/100 [24:18<28:23, 32.14s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '18/12/2020' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '18/12/2020' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:29:15,755 - INFO - Successfully saved article: 20201218_CNNOrganisedCrimeR1_child-sex-trafficking-it-s-probably-not-what-you-t.txt\n",
      "Extracting articles:  48%|████▊     | 48/100 [24:53<28:27, 32.84s/it]2025-03-17 21:29:34,441 - INFO - Successfully saved article: 20241212_CNNOrganisedCrimeR1_two-prominent-real-estate-brokers-and-their-brothe.txt\n",
      "Extracting articles:  49%|████▉     | 49/100 [25:11<24:18, 28.59s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '14/02/2025' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '14/02/2025' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:30:08,663 - INFO - Successfully saved article: 20250214_CNNOrganisedCrimeR1_3-men-found-guilty-of-murder-in-drugging-robbery-s.txt\n",
      "Extracting articles:  50%|█████     | 50/100 [25:46<25:14, 30.28s/it]2025-03-17 21:30:45,495 - INFO - Successfully saved article: 20230911_CNNOrganisedCrimeR1_doj-announces-arrests-in-high-end-brothel-network.txt\n",
      "Extracting articles:  51%|█████     | 51/100 [26:22<26:20, 32.25s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '24/09/2024' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '24/09/2024' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:31:25,103 - INFO - Successfully saved article: 20240924_CNNOrganisedCrimeR1_violent-crime-dropped-an-estimated-3-in-the-united.txt\n",
      "Extracting articles:  52%|█████▏    | 52/100 [27:02<27:33, 34.46s/it]2025-03-17 21:32:00,067 - INFO - Successfully saved article: 20220704_CNNOrganisedCrimeR1_man-who-lived-in-daughter-s-sarah-lawrence-college.txt\n",
      "Extracting articles:  53%|█████▎    | 53/100 [27:37<27:06, 34.61s/it]2025-03-17 21:32:35,290 - INFO - Successfully saved article: 20230616_CNNOrganisedCrimeR1_us-state-department-identifies-rapidly-growing-and.txt\n",
      "Extracting articles:  54%|█████▍    | 54/100 [28:12<26:40, 34.79s/it]2025-03-17 21:33:12,369 - INFO - Successfully saved article: 20231201_CNNOrganisedCrimeR1_4-gun-traffickers-charged-in-new-york-marking-the.txt\n",
      "Extracting articles:  55%|█████▌    | 55/100 [28:49<26:36, 35.48s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '27/06/2024' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '27/06/2024' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:33:53,509 - INFO - Successfully saved article: 20240627_CNNOrganisedCrimeR1_icc-convicts-al-qaeda-linked-leader-of-crimes-agai.txt\n",
      "Extracting articles:  56%|█████▌    | 56/100 [29:30<27:15, 37.18s/it]2025-03-17 21:34:26,542 - INFO - Successfully saved article: 20230504_CNNOrganisedCrimeR1_4-men-have-been-arrested-in-connection-with-robber.txt\n",
      "Extracting articles:  57%|█████▋    | 57/100 [30:04<25:45, 35.93s/it]2025-03-17 21:34:59,557 - INFO - Successfully saved article: 20240510_CNNOrganisedCrimeR1_the-menendez-brothers-case-is-not-the-only-one-tha.txt\n",
      "Extracting articles:  58%|█████▊    | 58/100 [30:37<24:32, 35.06s/it]2025-03-17 21:35:32,797 - INFO - Successfully saved article: 20230808_CNNOrganisedCrimeR1_dozens-arrested-over-alleged-child-sex-abuse-follo.txt\n",
      "Extracting articles:  59%|█████▉    | 59/100 [31:10<23:35, 34.51s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '21/08/2023' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '21/08/2023' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:36:06,621 - INFO - Successfully saved article: 20230821_CNNOrganisedCrimeR1_drug-dealer-sentenced-to-10-years-in-prison-for-de.txt\n",
      "Extracting articles:  60%|██████    | 60/100 [31:44<22:52, 34.31s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '17/11/2023' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '17/11/2023' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:36:10,723 - INFO - Successfully saved article: 20231117_CNNOrganisedCrimeR1_what-america-s-shoplifting-panic-is-really-about.txt\n",
      "Extracting articles:  61%|██████    | 61/100 [31:48<16:24, 25.24s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '19/04/2023' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '19/04/2023' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:36:44,834 - INFO - Successfully saved article: 20230419_CNNOrganisedCrimeR1_5-men-were-indicted-in-connection-with-nyc-bar-rob.txt\n",
      "Extracting articles:  62%|██████▏   | 62/100 [32:22<17:40, 27.90s/it]2025-03-17 21:37:20,657 - INFO - Successfully saved article: 20241106_CNNOrganisedCrimeR1_violent-crime-is-down-and-the-us-murder-rate-is-pl.txt\n",
      "Extracting articles:  63%|██████▎   | 63/100 [32:58<18:40, 30.28s/it]2025-03-17 21:37:54,499 - INFO - Successfully saved article: 20230409_CNNOrganisedCrimeR1_drugs-are-sold-out-in-the-open-in-san-francisco-s.txt\n",
      "Extracting articles:  64%|██████▍   | 64/100 [33:31<18:48, 31.35s/it]2025-03-17 21:38:30,933 - INFO - Successfully saved article: 20241212_CNNOrganisedCrimeR1_video-points-to-assad-regime-s-involvement-in-larg.txt\n",
      "Extracting articles:  65%|██████▌   | 65/100 [34:08<19:10, 32.87s/it]2025-03-17 21:39:04,237 - INFO - Successfully saved article: 20210722_CNNOrganisedCrimeR1_ad-hoc-extremist-groups-come-into-focus-in-post-ja.txt\n",
      "Extracting articles:  66%|██████▌   | 66/100 [34:41<18:42, 33.00s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '20/02/2024' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '20/02/2024' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:39:36,620 - INFO - Successfully saved article: 20240220_CNNOrganisedCrimeR1_-it-s-definitely-a-crisis-this-is-the-reality-for.txt\n",
      "Extracting articles:  67%|██████▋   | 67/100 [35:14<18:02, 32.82s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '29/10/2023' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '29/10/2023' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:39:40,963 - INFO - Successfully saved article: 20231029_CNNOrganisedCrimeR1_abercrombie-fitch-facilitated-ex-ceo-s-sexual-expl.txt\n",
      "Extracting articles:  68%|██████▊   | 68/100 [35:18<12:56, 24.28s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '24/10/2024' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '24/10/2024' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:39:46,074 - INFO - Successfully saved article: 20241024_CNNOrganisedCrimeR1_former-ceo-of-abercrombie-fitch-indicted-on-sex-tr.txt\n",
      "Extracting articles:  69%|██████▉   | 69/100 [35:23<09:34, 18.53s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '30/07/2020' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '30/07/2020' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:40:21,126 - INFO - Successfully saved article: 20200730_CNNOrganisedCrimeR1_reputed-head-of-black-disciples-gang-in-chicago-am.txt\n",
      "Extracting articles:  70%|███████   | 70/100 [35:58<11:44, 23.48s/it]2025-03-17 21:40:54,370 - INFO - Successfully saved article: 20241009_CNNOrganisedCrimeR1_alleged-leaders-of-white-supremacist-group-charged.txt\n",
      "Extracting articles:  71%|███████   | 71/100 [36:31<12:45, 26.41s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '19/11/2021' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '19/11/2021' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:41:28,812 - INFO - Successfully saved article: 20211119_CNNOrganisedCrimeR1_us-announces-sex-trafficking-charges-against-duter.txt\n",
      "Extracting articles:  72%|███████▏  | 72/100 [37:06<13:26, 28.82s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '23/12/2023' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '23/12/2023' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:41:33,504 - INFO - Successfully saved article: 20231223_CNNOrganisedCrimeR1_pornhub-s-parent-company-admits-it-profited-from-s.txt\n",
      "Extracting articles:  73%|███████▎  | 73/100 [37:10<09:42, 21.58s/it]2025-03-17 21:42:07,716 - INFO - Successfully saved article: 20210311_CNNOrganisedCrimeR1_doj-charges-13-alleged-members-of-violent-chicago.txt\n",
      "Extracting articles:  74%|███████▍  | 74/100 [37:45<10:59, 25.37s/it]2025-03-17 21:42:40,061 - INFO - Successfully saved article: 20220612_CNNOrganisedCrimeR1_documentary-chronicles-an-unexpected-side-of-priso.txt\n",
      "Extracting articles:  75%|███████▌  | 75/100 [38:17<11:26, 27.46s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '28/01/2022' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '28/01/2022' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:43:14,149 - INFO - Successfully saved article: 20220128_CNNOrganisedCrimeR1_why-rising-crime-isn-t-breaking-through-as-a-natio.txt\n",
      "Extracting articles:  76%|███████▌  | 76/100 [38:51<11:46, 29.45s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '16/12/2024' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '16/12/2024' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:43:44,858 - INFO - Successfully saved article: 20241216_APOrganisedCrimeR1_a-suburban-police-force-in-new-york-strip-searched.txt\n",
      "Extracting articles:  77%|███████▋  | 77/100 [39:22<11:26, 29.83s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '26/10/2024' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '26/10/2024' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:44:17,643 - INFO - Successfully saved article: 20241026_CNNOrganisedCrimeR1_the-fbi-releases-crime-stats-months-after-the-fact.txt\n",
      "Extracting articles:  78%|███████▊  | 78/100 [39:55<11:15, 30.71s/it]2025-03-17 21:44:49,690 - INFO - Successfully saved article: 20200810_CNNOrganisedCrimeR1_new-nollywood-film-shines-a-light-on-human-traffic.txt\n",
      "Extracting articles:  79%|███████▉  | 79/100 [40:27<10:53, 31.11s/it]2025-03-17 21:45:25,060 - INFO - Successfully saved article: 20220806_CNNOrganisedCrimeR1_north-carolina-man-indicted-in-nyc-for-trafficking.txt\n",
      "Extracting articles:  80%|████████  | 80/100 [41:02<10:47, 32.39s/it]2025-03-17 21:45:57,880 - INFO - Successfully saved article: 20240403_CNNOrganisedCrimeR1_india-arrests-three-men-for-alleged-gang-rape-of-f.txt\n",
      "Extracting articles:  81%|████████  | 81/100 [41:35<10:17, 32.52s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '15/12/2021' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '15/12/2021' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:46:32,580 - INFO - Successfully saved article: 20211215_CNNOrganisedCrimeR1_six-fbi-agents-investigated-for-allegedly-soliciti.txt\n",
      "Extracting articles:  82%|████████▏ | 82/100 [42:10<09:57, 33.17s/it]2025-03-17 21:46:37,323 - INFO - Successfully saved article: 20210712_CNNOrganisedCrimeR1_aligning-anti-trafficking-programs-with-un-sustain.txt\n",
      "Extracting articles:  83%|████████▎ | 83/100 [42:14<06:58, 24.64s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '25/07/2023' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '25/07/2023' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:47:11,839 - INFO - Successfully saved article: 20230725_CNNOrganisedCrimeR1_what-causes-someone-to-become-a-serial-killer-it-s.txt\n",
      "Extracting articles:  84%|████████▍ | 84/100 [42:49<07:21, 27.61s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '28/12/2021' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '28/12/2021' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:47:47,822 - INFO - Successfully saved article: 20211228_CNNOrganisedCrimeR1_justice-department-boosts-funds-to-cities-to-battl.txt\n",
      "Extracting articles:  85%|████████▌ | 85/100 [43:25<07:31, 30.12s/it]2025-03-17 21:48:20,388 - INFO - Successfully saved article: 20231202_CNNOrganisedCrimeR1_-don-t-let-another-sister-suffer-alleged-gang-rape.txt\n",
      "Extracting articles:  86%|████████▌ | 86/100 [43:57<07:11, 30.85s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '25/09/2024' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '25/09/2024' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:48:25,235 - INFO - Successfully saved article: 20240925_CNNOrganisedCrimeR1_did-you-know-violent-crime-is-down-not-if-you-re-w.txt\n",
      "Extracting articles:  87%|████████▋ | 87/100 [44:02<04:59, 23.05s/it]2025-03-17 21:48:57,095 - INFO - Successfully saved article: 20240503_CNNOrganisedCrimeR1_-the-program-takes-a-deeply-personal-look-at-the-t.txt\n",
      "Extracting articles:  88%|████████▊ | 88/100 [44:34<05:08, 25.69s/it]2025-03-17 21:49:30,359 - INFO - Successfully saved article: 20230412_CNNOrganisedCrimeR1_french-police-arrest-yoga-guru-in-connection-with.txt\n",
      "Extracting articles:  89%|████████▉ | 89/100 [45:07<05:07, 27.97s/it]2025-03-17 21:50:02,822 - INFO - Successfully saved article: 20230405_CNNOrganisedCrimeR1_what-s-really-going-on-with-crime-in-san-francisco.txt\n",
      "Extracting articles:  90%|█████████ | 90/100 [45:40<04:53, 29.31s/it]2025-03-17 21:50:36,764 - INFO - Successfully saved article: 20220206_CNNOrganisedCrimeR1_alleged-gang-rape-of-woman-on-moving-train-sparks.txt\n",
      "Extracting articles:  91%|█████████ | 91/100 [46:14<04:36, 30.70s/it]2025-03-17 21:51:08,623 - INFO - Successfully saved article: 20211005_CNNOrganisedCrimeR1_-the-crime-of-the-century-lays-bare-who-cashed-in.txt\n",
      "Extracting articles:  92%|█████████▏| 92/100 [46:46<04:08, 31.05s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '31/08/2023' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '31/08/2023' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:51:43,191 - INFO - Successfully saved article: 20230831_CNNOrganisedCrimeR1_connecticut-man-who-feds-say-committed-dozens-of-r.txt\n",
      "Extracting articles:  93%|█████████▎| 93/100 [47:20<03:44, 32.10s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '21/01/2023' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '21/01/2023' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:52:17,221 - INFO - Successfully saved article: 20230121_CNNOrganisedCrimeR1_rapper-young-thug-and-co-defendant-conducted-in-co.txt\n",
      "Extracting articles:  94%|█████████▍| 94/100 [47:54<03:16, 32.68s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '17/01/2025' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '17/01/2025' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:52:51,298 - INFO - Successfully saved article: 20250117_CNNOrganisedCrimeR1_biden-commutes-sentences-for-nearly-2-500-non-viol.txt\n",
      "Extracting articles:  95%|█████████▌| 95/100 [48:28<02:45, 33.10s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '27/06/2023' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '27/06/2023' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:53:26,492 - INFO - Successfully saved article: 20230627_CNNOrganisedCrimeR1_club-q-mass-shooter-sentenced-to-over-2-000-years.txt\n",
      "Extracting articles:  96%|█████████▌| 96/100 [49:03<02:14, 33.73s/it]2025-03-17 21:54:02,008 - INFO - Successfully saved article: 20210207_CNNOrganisedCrimeR1_covid-19-pandemic-increased-number-of-people-at-ri.txt\n",
      "Extracting articles:  97%|█████████▋| 97/100 [49:39<01:42, 34.26s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '29/11/2024' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '29/11/2024' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:54:35,576 - INFO - Successfully saved article: 20241129_CNNOrganisedCrimeR1_-narco-sub-bound-for-australia-seized-in-massive-i.txt\n",
      "Extracting articles:  98%|█████████▊| 98/100 [50:13<01:08, 34.06s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '14/07/2023' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '14/07/2023' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:55:08,674 - INFO - Successfully saved article: 20230714_CNNOrganisedCrimeR1_former-louisiana-priest-sentenced-to-25-years-afte.txt\n",
      "Extracting articles:  99%|█████████▉| 99/100 [50:46<00:33, 33.77s/it]C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1137: UserWarning: Parsing '21/06/2023' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(article.get('Date'))\n",
      "C:\\Users\\Bertrand Tan\\AppData\\Local\\Temp\\ipykernel_31372\\3883342777.py:1225: UserWarning: Parsing '21/06/2023' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  date_obj = pd.to_datetime(date_part)\n",
      "2025-03-17 21:55:40,919 - INFO - Successfully saved article: 20230621_CNNOrganisedCrimeR1_andrew-tate-indicted-on-human-trafficking-and-rape.txt\n",
      "Extracting articles: 100%|██████████| 100/100 [51:18<00:00, 30.78s/it]\n",
      "2025-03-17 21:55:40,923 - INFO - Saving updated Excel file: CNN_Organised_Crime_Articles_ProcessedUpdated.xlsx\n",
      "2025-03-17 21:55:41,059 - INFO - Successfully saved to CNN_Organised_Crime_Articles_ProcessedUpdated.xlsx\n",
      "2025-03-17 21:55:41,062 - INFO - Extraction complete: 99/100 articles extracted\n",
      "2025-03-17 21:55:41,063 - INFO - Articles saved to extracted_articles\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Orange\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3406: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8692e89a-56fc-4650-b6f7-30e29d8de582",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c6cc1f-2f62-4479-8bcd-d472ae120c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Orange]",
   "language": "python",
   "name": "conda-env-Orange-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
