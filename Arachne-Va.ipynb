{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6270960c-7291-4605-8ede-09bf8ef38160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\program files\\orange\\lib\\site-packages (1.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\program files\\orange\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\bertrand tan\\appdata\\roaming\\python\\python38\\site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\program files\\orange\\lib\\site-packages (from pandas) (2022.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\program files\\orange\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in c:\\program files\\orange\\lib\\site-packages (2.28.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\program files\\orange\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\program files\\orange\\lib\\site-packages (from requests) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\program files\\orange\\lib\\site-packages (from requests) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\program files\\orange\\lib\\site-packages (from requests) (1.26.11)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\bertrand tan\\appdata\\roaming\\python\\python38\\site-packages (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\bertrand tan\\appdata\\roaming\\python\\python38\\site-packages (from beautifulsoup4) (2.3.2.post1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: lxml in c:\\users\\bertrand tan\\appdata\\roaming\\python\\python38\\site-packages (4.9.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: html5lib in c:\\users\\bertrand tan\\appdata\\roaming\\python\\python38\\site-packages (1.1)\n",
      "Requirement already satisfied: webencodings in c:\\users\\bertrand tan\\appdata\\roaming\\python\\python38\\site-packages (from html5lib) (0.5.1)\n",
      "Requirement already satisfied: six>=1.9 in c:\\program files\\orange\\lib\\site-packages (from html5lib) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in c:\\users\\bertrand tan\\appdata\\roaming\\python\\python38\\site-packages (4.27.1)\n",
      "Requirement already satisfied: webdriver-manager in c:\\users\\bertrand tan\\appdata\\roaming\\python\\python38\\site-packages (4.0.2)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\bertrand tan\\appdata\\roaming\\python\\python38\\site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\program files\\orange\\lib\\site-packages (from selenium) (1.26.11)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\bertrand tan\\appdata\\roaming\\python\\python38\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\program files\\orange\\lib\\site-packages (from selenium) (2022.9.24)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\bertrand tan\\appdata\\roaming\\python\\python38\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\bertrand tan\\appdata\\roaming\\python\\python38\\site-packages (from selenium) (0.27.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\bertrand tan\\appdata\\roaming\\python\\python38\\site-packages (from webdriver-manager) (1.0.1)\n",
      "Requirement already satisfied: requests in c:\\program files\\orange\\lib\\site-packages (from webdriver-manager) (2.28.1)\n",
      "Requirement already satisfied: packaging in c:\\program files\\orange\\lib\\site-packages (from webdriver-manager) (21.3)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\bertrand tan\\appdata\\roaming\\python\\python38\\site-packages (from trio~=0.17->selenium) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\program files\\orange\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\program files\\orange\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: outcome in c:\\users\\bertrand tan\\appdata\\roaming\\python\\python38\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\bertrand tan\\appdata\\roaming\\python\\python38\\site-packages (from trio~=0.17->selenium) (25.1.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\bertrand tan\\appdata\\roaming\\python\\python38\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\program files\\orange\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\bertrand tan\\appdata\\roaming\\python\\python38\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\program files\\orange\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\program files\\orange\\lib\\site-packages (from packaging->webdriver-manager) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\program files\\orange\\lib\\site-packages (from requests->webdriver-manager) (2.1.1)\n",
      "Requirement already satisfied: pycparser in c:\\program files\\orange\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\program files\\orange\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install requests\n",
    "!pip install beautifulsoup4\n",
    "!pip install lxml\n",
    "!pip install html5lib\n",
    "!pip install selenium webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f50e298f-ecb6-4335-9267-3240ffc76767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import logging\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "import time\n",
    "import csv\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9bfe7675-d1c4-44fa-911e-454d589503a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, \n",
    "    format='%(asctime)s - %(levelname)s: %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('cna_scraper_debug.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "310c351d-43ac-417e-99c9-98086083f999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_webdriver():\n",
    "    \"\"\"\n",
    "    Set up Selenium WebDriver with Chrome and comprehensive error handling\n",
    "    \n",
    "    Returns:\n",
    "        webdriver: Configured Chrome WebDriver\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Chrome options\n",
    "        chrome_options = Options()\n",
    "        \n",
    "        # More advanced anti-detection techniques\n",
    "        chrome_options.add_argument('--no-sandbox')\n",
    "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "        chrome_options.add_argument('--disable-gpu')\n",
    "        chrome_options.add_argument('--window-size=1920,1080')\n",
    "        chrome_options.add_argument('--ignore-certificate-errors')\n",
    "        chrome_options.add_argument('--allow-running-insecure-content')\n",
    "        \n",
    "        # Advanced anti-detection options\n",
    "        chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "        chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "        \n",
    "        # More sophisticated user agent\n",
    "        chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')\n",
    "        \n",
    "        # Attempt to install and configure WebDriver\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        \n",
    "        # Create WebDriver with extensive error handling\n",
    "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "        \n",
    "        # Additional configurations\n",
    "        driver.set_page_load_timeout(60)  # Increased timeout\n",
    "        \n",
    "        # Additional anti-detection step\n",
    "        driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', { get: () => undefined })\")\n",
    "        \n",
    "        logging.info(\"WebDriver successfully initialized\")\n",
    "        return driver\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to initialize WebDriver: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "961bcb17-75e8-4aef-969c-c9193635f290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_cna_search_results(driver, url):\n",
    "    \"\"\"\n",
    "    Scrape article details from CNA search results page using Selenium\n",
    "    \n",
    "    Args:\n",
    "        driver (webdriver): Selenium WebDriver\n",
    "        url (str): URL of CNA search results page\n",
    "    \n",
    "    Returns:\n",
    "        list: List of dictionaries containing article details\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Navigate to the URL with more verbose logging\n",
    "        logging.info(f\"Attempting to navigate to URL: {url}\")\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Add more comprehensive wait and debug logging\n",
    "        try:\n",
    "            WebDriverWait(driver, 60).until(\n",
    "                lambda d: d.execute_script('return document.readyState') == 'complete'\n",
    "            )\n",
    "            logging.info(\"Page fully loaded\")\n",
    "        except Exception as wait_error:\n",
    "            logging.error(f\"Page load wait failed: {wait_error}\")\n",
    "            # Take a screenshot for debugging\n",
    "            driver.save_screenshot(f'debug_screenshot_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.png')\n",
    "        \n",
    "        # Extended wait and multiple selector attempts\n",
    "        selectors_to_try = [\n",
    "            '.list-object__heading',\n",
    "            '.ais-list-object',\n",
    "            '.search-results-list .item',\n",
    "            'div[data-testid=\"search-result-item\"]'\n",
    "        ]\n",
    "        \n",
    "        article_links = []\n",
    "        for selector in selectors_to_try:\n",
    "            try:\n",
    "                article_links = driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                if article_links:\n",
    "                    logging.info(f\"Found {len(article_links)} articles using selector: {selector}\")\n",
    "                    break\n",
    "            except Exception as sel_error:\n",
    "                logging.warning(f\"Selector {selector} failed: {sel_error}\")\n",
    "        \n",
    "        if not article_links:\n",
    "            logging.error(\"No articles found using any selectors\")\n",
    "            # Take a screenshot for debugging\n",
    "            driver.save_screenshot(f'debug_no_articles_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.png')\n",
    "            return []\n",
    "        \n",
    "        # Extract article details with more robust error handling\n",
    "        articles = []\n",
    "        for link in article_links:\n",
    "            try:\n",
    "                # More flexible title extraction\n",
    "                title_selectors = [\n",
    "                    '.a-list-object__heading-link',\n",
    "                    '.list-object__heading-link',\n",
    "                    'a.title',\n",
    "                    'h3 a',\n",
    "                    '.search-result-title'\n",
    "                ]\n",
    "                \n",
    "                title = ''\n",
    "                article_url = 'N/A'\n",
    "                for title_selector in title_selectors:\n",
    "                    try:\n",
    "                        title_elem = link.find_element(By.CSS_SELECTOR, title_selector)\n",
    "                        title = title_elem.text.strip()\n",
    "                        article_url = title_elem.get_attribute('href')\n",
    "                        if title and article_url:\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                # Skip articles with empty titles\n",
    "                if not title:\n",
    "                    continue\n",
    "                \n",
    "                # More flexible date extraction\n",
    "                date_selectors = [\n",
    "                    '.hit-date',\n",
    "                    'list-object__timestamp'\n",
    "                    '.timestamp',\n",
    "                    '.search-result-date',\n",
    "                    '.article-date'\n",
    "                ]\n",
    "                \n",
    "                article_date = 'N/A'\n",
    "                for date_selector in date_selectors:\n",
    "                    try:\n",
    "                        date_elem = link.find_element(By.CSS_SELECTOR, date_selector)\n",
    "                        article_date = date_elem.text.strip()\n",
    "                        if article_date:\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                # More flexible description extraction\n",
    "                desc_selectors = [\n",
    "                    '.hit-description',\n",
    "                    '.search-result-description',\n",
    "                    '.article-excerpt',\n",
    "                    'p.description'\n",
    "                ]\n",
    "                \n",
    "                description = 'N/A'\n",
    "                for desc_selector in desc_selectors:\n",
    "                    try:\n",
    "                        desc_elem = link.find_element(By.CSS_SELECTOR, desc_selector)\n",
    "                        description = desc_elem.text.strip()\n",
    "                        if description:\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                # Create article dictionary\n",
    "                article = {\n",
    "                    'Title': title,\n",
    "                    'URL': article_url,\n",
    "                    'Date': article_date,\n",
    "                    'Description': description\n",
    "                }\n",
    "                \n",
    "                articles.append(article)\n",
    "            \n",
    "            except Exception as elem_error:\n",
    "                logging.warning(f\"Error extracting individual article: {elem_error}\")\n",
    "        \n",
    "        # Only save if we have articles\n",
    "        if articles:\n",
    "            # Generate a timestamped filename\n",
    "            output_filename = f'cna_articles_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "            \n",
    "            # Save to CSV with only non-empty title rows\n",
    "            articles_df = pd.DataFrame(articles)\n",
    "            articles_df = articles_df[articles_df['Title'].str.strip() != '']\n",
    "            articles_df.to_csv(output_filename, index=False, encoding='utf-8')\n",
    "            \n",
    "            logging.info(f\"Saved {len(articles_df)} articles to {output_filename}\")\n",
    "        else:\n",
    "            logging.warning(\"No articles could be extracted\")\n",
    "        \n",
    "        return articles\n",
    "    \n",
    "    except Exception as error:\n",
    "        logging.error(f\"Critical error scraping URL {url}: {error}\")\n",
    "        # Take a screenshot for debugging\n",
    "        try:\n",
    "            driver.save_screenshot(f'debug_critical_error_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.png')\n",
    "        except:\n",
    "            pass\n",
    "        return []\n",
    "    \"\"\"\n",
    "    Scrape article details from CNA search results page using Selenium\n",
    "    \n",
    "    Args:\n",
    "        driver (webdriver): Selenium WebDriver\n",
    "        url (str): URL of CNA search results page\n",
    "    \n",
    "    Returns:\n",
    "        list: List of dictionaries containing article details\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Navigate to the URL with more verbose logging\n",
    "        logging.info(f\"Attempting to navigate to URL: {url}\")\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Add more comprehensive wait and debug logging\n",
    "        try:\n",
    "            WebDriverWait(driver, 60).until(\n",
    "                lambda d: d.execute_script('return document.readyState') == 'complete'\n",
    "            )\n",
    "            logging.info(\"Page fully loaded\")\n",
    "        except Exception as wait_error:\n",
    "            logging.error(f\"Page load wait failed: {wait_error}\")\n",
    "            # Take a screenshot for debugging\n",
    "            driver.save_screenshot(f'debug_screenshot_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.png')\n",
    "        \n",
    "        # Extended wait and multiple selector attempts\n",
    "        selectors_to_try = [\n",
    "            '.ais-list-object',\n",
    "            '.search-results-list .item',\n",
    "            'div[data-testid=\"search-result-item\"]'\n",
    "        ]\n",
    "        \n",
    "        article_links = []\n",
    "        for selector in selectors_to_try:\n",
    "            try:\n",
    "                article_links = driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                if article_links:\n",
    "                    logging.info(f\"Found {len(article_links)} articles using selector: {selector}\")\n",
    "                    break\n",
    "            except Exception as sel_error:\n",
    "                logging.warning(f\"Selector {selector} failed: {sel_error}\")\n",
    "        \n",
    "        if not article_links:\n",
    "            logging.error(\"No articles found using any selectors\")\n",
    "            # Take a screenshot for debugging\n",
    "            driver.save_screenshot(f'debug_no_articles_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.png')\n",
    "            return []\n",
    "        \n",
    "        # Extract article details with more robust error handling\n",
    "        articles = []\n",
    "        for link in article_links:\n",
    "            try:\n",
    "                # More flexible title extraction\n",
    "                title_selectors = [\n",
    "                    '.a-list-object__heading-link',\n",
    "                    'a.title',\n",
    "                    'h3 a',\n",
    "                    '.search-result-title'\n",
    "                ]\n",
    "                \n",
    "                title = 'N/A'\n",
    "                article_url = 'N/A'\n",
    "                for title_selector in title_selectors:\n",
    "                    try:\n",
    "                        title_elem = link.find_element(By.CSS_SELECTOR, title_selector)\n",
    "                        title = title_elem.text.strip()\n",
    "                        article_url = title_elem.get_attribute('href')\n",
    "                        if title and article_url:\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                # More flexible date extraction\n",
    "                date_selectors = [\n",
    "                    '.hit-date',\n",
    "                    '.timestamp',\n",
    "                    '.search-result-date',\n",
    "                    '.article-date'\n",
    "                ]\n",
    "                \n",
    "                article_date = 'N/A'\n",
    "                for date_selector in date_selectors:\n",
    "                    try:\n",
    "                        date_elem = link.find_element(By.CSS_SELECTOR, date_selector)\n",
    "                        article_date = date_elem.text.strip()\n",
    "                        if article_date:\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                # More flexible description extraction\n",
    "                desc_selectors = [\n",
    "                    '.hit-description',\n",
    "                    '.search-result-description',\n",
    "                    '.article-excerpt',\n",
    "                    'p.description'\n",
    "                ]\n",
    "                \n",
    "                description = 'N/A'\n",
    "                for desc_selector in desc_selectors:\n",
    "                    try:\n",
    "                        desc_elem = link.find_element(By.CSS_SELECTOR, desc_selector)\n",
    "                        description = desc_elem.text.strip()\n",
    "                        if description:\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                # Create article dictionary\n",
    "                article = {\n",
    "                    'Title': title,\n",
    "                    'URL': article_url,\n",
    "                    'Date': article_date,\n",
    "                    'Description': description\n",
    "                }\n",
    "                \n",
    "                articles.append(article)\n",
    "            \n",
    "            except Exception as elem_error:\n",
    "                logging.warning(f\"Error extracting individual article: {elem_error}\")\n",
    "        \n",
    "        # Only save if we have articles\n",
    "        if articles:\n",
    "            # Generate a timestamped filename\n",
    "            output_filename = f'cna_articles_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "            \n",
    "            # Save to CSV\n",
    "            articles_df = pd.DataFrame(articles)\n",
    "            articles_df.to_csv(output_filename, index=False, encoding='utf-8')\n",
    "            \n",
    "            logging.info(f\"Saved {len(articles)} articles to {output_filename}\")\n",
    "        else:\n",
    "            logging.warning(\"No articles could be extracted\")\n",
    "        \n",
    "        return articles\n",
    "    \n",
    "    except Exception as error:\n",
    "        logging.error(f\"Critical error scraping URL {url}: {error}\")\n",
    "        # Take a screenshot for debugging\n",
    "        try:\n",
    "            driver.save_screenshot(f'debug_critical_error_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.png')\n",
    "        except:\n",
    "            pass\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d443e1da-3358-4d23-83e2-3b903a1b1fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to process URLs and scrape search results\n",
    "    \"\"\"\n",
    "    # Set up WebDriver\n",
    "    driver = setup_webdriver()\n",
    "    \n",
    "    try:\n",
    "        # Read URLs from CSV\n",
    "        urls_df = pd.read_csv('CNA_Organised_Crime_URLs.csv')\n",
    "        \n",
    "        # Will store articles from all URLs\n",
    "        all_articles = []\n",
    "        \n",
    "        # Iterate through URLs\n",
    "        for index, row in urls_df.iterrows():\n",
    "            url = row['URL']\n",
    "            logging.info(f\"Scraping URL: {url}\")\n",
    "            \n",
    "            # Scrape and collect articles\n",
    "            articles = scrape_cna_search_results(driver, url)\n",
    "            all_articles.extend(articles)\n",
    "            \n",
    "            # Be nice to the server - add a delay\n",
    "            time.sleep(2)\n",
    "        \n",
    "        # Create a consolidated CSV of all articles\n",
    "        if all_articles:\n",
    "            consolidated_df = pd.DataFrame(all_articles)\n",
    "            consolidated_filename = f'cna_all_articles_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "            consolidated_df.to_csv(consolidated_filename, index=False, encoding='utf-8')\n",
    "            logging.info(f\"Saved total {len(all_articles)} articles to {consolidated_filename}\")\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        logging.error(\"Error: CNA_Organised_Crime_URLs.csv file not found.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        # Always close the driver\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d844eaa0-de04-4932-9b7d-0fa551170f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 09:46:48,536 - INFO: ====== WebDriver manager ======\n",
      "2025-03-06 09:46:50,595 - INFO: Get LATEST chromedriver version for google-chrome\n",
      "2025-03-06 09:46:51,376 - INFO: Get LATEST chromedriver version for google-chrome\n",
      "2025-03-06 09:46:51,854 - INFO: Get LATEST chromedriver version for google-chrome\n",
      "2025-03-06 09:46:52,627 - INFO: WebDriver version 133.0.6943.141 selected\n",
      "2025-03-06 09:46:52,637 - INFO: Modern chrome version https://storage.googleapis.com/chrome-for-testing-public/133.0.6943.141/win32/chromedriver-win32.zip\n",
      "2025-03-06 09:46:52,640 - INFO: About to download new driver from https://storage.googleapis.com/chrome-for-testing-public/133.0.6943.141/win32/chromedriver-win32.zip\n",
      "2025-03-06 09:46:53,129 - INFO: Driver downloading response is 200\n",
      "2025-03-06 09:47:03,736 - INFO: Get LATEST chromedriver version for google-chrome\n",
      "2025-03-06 09:47:04,987 - INFO: Driver has been saved in cache [C:\\Users\\Bertrand Tan\\.wdm\\drivers\\chromedriver\\win64\\133.0.6943.141]\n",
      "2025-03-06 09:47:07,147 - INFO: WebDriver successfully initialized\n",
      "2025-03-06 09:47:07,161 - INFO: Scraping URL: https://www.channelnewsasia.com/search?q=%22Organsed%20Crime%22%20%22Drug%20Trafficking%22&type%5B0%5D=article&categories%5B0%5D=Asia&categories%5B1%5D=Big%20Read&categories%5B2%5D=Business&categories%5B3%5D=Commentary&categories%5B4%5D=East%20Asia&categories%5B5%5D=Singapore&categories%5B6%5D=Sport&categories%5B7%5D=Sustainability&categories%5B8%5D=World\n",
      "2025-03-06 09:47:07,161 - INFO: Attempting to navigate to URL: https://www.channelnewsasia.com/search?q=%22Organsed%20Crime%22%20%22Drug%20Trafficking%22&type%5B0%5D=article&categories%5B0%5D=Asia&categories%5B1%5D=Big%20Read&categories%5B2%5D=Business&categories%5B3%5D=Commentary&categories%5B4%5D=East%20Asia&categories%5B5%5D=Singapore&categories%5B6%5D=Sport&categories%5B7%5D=Sustainability&categories%5B8%5D=World\n",
      "2025-03-06 09:47:19,198 - INFO: Page fully loaded\n",
      "2025-03-06 09:47:19,310 - INFO: Found 30 articles using selector: .list-object__heading\n",
      "2025-03-06 09:47:22,228 - INFO: Saved 15 articles to cna_articles_20250306_094722.csv\n",
      "2025-03-06 09:47:24,235 - INFO: Scraping URL: https://www.channelnewsasia.com/search?q=%22Organsed%20Crime%22%20%22Drug%20Trafficking%22&type%5B0%5D=article&categories%5B0%5D=Asia&categories%5B1%5D=Big%20Read&categories%5B2%5D=Business&categories%5B3%5D=Commentary&categories%5B4%5D=East%20Asia&categories%5B5%5D=Singapore&categories%5B6%5D=Sport&categories%5B7%5D=Sustainability&categories%5B8%5D=World&page=2\n",
      "2025-03-06 09:47:24,235 - INFO: Attempting to navigate to URL: https://www.channelnewsasia.com/search?q=%22Organsed%20Crime%22%20%22Drug%20Trafficking%22&type%5B0%5D=article&categories%5B0%5D=Asia&categories%5B1%5D=Big%20Read&categories%5B2%5D=Business&categories%5B3%5D=Commentary&categories%5B4%5D=East%20Asia&categories%5B5%5D=Singapore&categories%5B6%5D=Sport&categories%5B7%5D=Sustainability&categories%5B8%5D=World&page=2\n",
      "2025-03-06 09:47:28,955 - INFO: Page fully loaded\n",
      "2025-03-06 09:47:28,968 - INFO: Found 30 articles using selector: .list-object__heading\n",
      "2025-03-06 09:47:32,315 - INFO: Saved 15 articles to cna_articles_20250306_094732.csv\n",
      "2025-03-06 09:47:34,325 - INFO: Scraping URL: https://www.channelnewsasia.com/search?q=%22Organsed%20Crime%22%20%22Drug%20Trafficking%22&type%5B0%5D=article&categories%5B0%5D=Asia&categories%5B1%5D=Big%20Read&categories%5B2%5D=Business&categories%5B3%5D=Commentary&categories%5B4%5D=East%20Asia&categories%5B5%5D=Singapore&categories%5B6%5D=Sport&categories%5B7%5D=Sustainability&categories%5B8%5D=World&page=3\n",
      "2025-03-06 09:47:34,327 - INFO: Attempting to navigate to URL: https://www.channelnewsasia.com/search?q=%22Organsed%20Crime%22%20%22Drug%20Trafficking%22&type%5B0%5D=article&categories%5B0%5D=Asia&categories%5B1%5D=Big%20Read&categories%5B2%5D=Business&categories%5B3%5D=Commentary&categories%5B4%5D=East%20Asia&categories%5B5%5D=Singapore&categories%5B6%5D=Sport&categories%5B7%5D=Sustainability&categories%5B8%5D=World&page=3\n",
      "2025-03-06 09:47:38,946 - INFO: Page fully loaded\n",
      "2025-03-06 09:47:38,963 - INFO: Found 26 articles using selector: .list-object__heading\n",
      "2025-03-06 09:47:40,977 - INFO: Saved 13 articles to cna_articles_20250306_094740.csv\n",
      "2025-03-06 09:47:42,985 - INFO: Saved total 43 articles to cna_all_articles_20250306_094742.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Orange]",
   "language": "python",
   "name": "conda-env-Orange-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
